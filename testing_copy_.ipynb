{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network\n",
    "\n",
    "Now that all the networks have been successfully trained, their models and training data in folders models, model_scores and model_infos we can commence testing.\n",
    "Each model will be tested on a 50-50 split of male to female testing data, solely male testing set and solely female testing set to compare between each other.\n",
    "\n",
    "First we will load all the models into memory, this is simply done by insert them all into a list and then initialising a model and then loading each file into that subsequent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding in model names\n",
    "model_names = [\"convNN2_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"dense_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\", \"resnet34_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"resnet34_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"resnet34_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\"]\n",
    "\n",
    "models = [convNN2(), convNN2(), convNN2(), denseNN(), denseNN(), denseNN(), resnet34(), resnet34(), resnet34()]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    model.load_state_dict(torch.load(\"./models/\" + name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Once we have done this we can now calculate the scores of each model on each test set, this should compute the mean error and std and return it as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    print(\"50-50 test file \" + evaluate(model, \"./ll_test_50-50\", \"cpu\"))\n",
    "    print(\"male test file \" + evaluate(model, \"./ll_test_males\", \"cpu\"))\n",
    "    print(\"female test file \" + evaluate(model, \"./ll_test_females\", \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error for the convNN2 is much higher than the the other two models, this is due to the simplicity of the model. \n",
    "While we can see that ResNet34 outperformed DesneNet121 with a significant reduction in error between the two.\n",
    "Both models had higher error in females for identifing landmarks for both genders, as expected started this project.\n",
    "\n",
    "Both models performed best overall on balanced data sets, though this difference was very slight.\n",
    "It also shows that a higher proportional of females or males in the dataset does not neccesarily result in a lower error for each respective gender, but the change in error was quite minimal \n",
    "meaning we can discard the bias in the dataset as a source of the bias between males and females.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate graphs for the change in mean error and standard deviation during training, and how it progressed over the iterations.\n",
    "\n",
    "All the extra saved data, when the model is loaded in and plotted on two seperate graphs.\n",
    "When loading in the model scores file, we need to parse this data to make it useful to us and is split into a list by using ',' as dividers.\n",
    "While the model infos are already presented in a json file and ready to read in simply, this gives us the iteration and mean or std of the saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add Loss for each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in models:\n",
    "    epoch = []\n",
    "    error = []\n",
    "    std = []\n",
    "    m_nopt = args.model.split(\".pt\")\n",
    "    with open(\"./model_scores/\" + m_nopt[0] + \".csv\") as file:\n",
    "        for line in file:\n",
    "            scores = line.split(\",\")   \n",
    "            epoch.append(float(scores[0]))\n",
    "            error.append(float(scores[1]))\n",
    "            std.append(float(scores[2]))\n",
    "    \n",
    "    file = open(\"./model_infos/\" + m_nopt[0] + \".json\")\n",
    "    model_info = json.load(file)\n",
    "\n",
    "    plt.plot(epoch, error)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Error\")\n",
    "    plt.title('Error change during training')\n",
    "\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"mean\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch, std)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Standard Deviation\")\n",
    "    plt.title('STD change during training')\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"std\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we can see that all error all std over time rapidly decreases within the first few iterations as the weights start off as zero, and the furthest possible away from the correct points (chin, nose and left eye) as possible.\n",
    "Both the ConvNN2 and DenseNet seem to oscillate between certain points and do not converge to a single point like the Resnet34 does, this seems to indicate that they have not obtained the most optimal model.\n",
    "\n",
    "We can visually check how accurate and close the predictions of the landmarks are on an image, comparing these against the input landmarks used by the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(train_dataloader, axs_flat):\n",
    "    with torch.no_grad():\n",
    "            for i, (image, _, _, _, labels) in enumerate(train_dataloader):\n",
    "                output = model(image)\n",
    "                output = output.reshape(3,2)\n",
    "                image = image.squeeze()\n",
    "                image = image.permute(1, 2, 0)    #Default was 3,200,200\n",
    "                im = axs_flat[i].imshow(image)\n",
    "                x = np.array(range(200))\n",
    "\n",
    "                # Finding landmarks for chin, nose and eye this txt file for each image\n",
    "                land_idx = [8, 30, 39]\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels[land_idx, :]\n",
    "                #ax.scatter(output[:,0], output[:,1], linewidth=2, color='red')\n",
    "                axs_flat[i].scatter(output[:,0], output[:,1], linewidth=2, color='c', s = 5)\n",
    "                axs_flat[i].scatter(labels[:,0], labels[:,1], linewidth=2, color='m', s = 5)\n",
    "\n",
    "UTKFace = CustomImageDataset(\"./test_output_images\", 'UTKFace')\n",
    "train_dataloader = DataLoader(UTKFace, \n",
    "                                    batch_size=1, \n",
    "                                    shuffle=False)\n",
    "\n",
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    fig, axs = plt.subplots(2,5, figsize=(20,10))\n",
    "    axs_flat = axs.flatten()\n",
    "\n",
    "    generate_images(train_dataloader, axs_flat)\n",
    "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.legend(('Predicted output','Expected output'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "As we can see the basic convolutional neural network essentially predicts the same three points for every single image, since that gives the best average mean error for the model\n",
    "as the model is underfitting. Since it doesn't have enough layers, depth or complexity in order to compute, recognise and plots the landmarks on different features of the face. \n",
    "\n",
    "Currently the model produces a very high mean error 54.377 and 54.895 for females and males, and in a 200x200 image this is very far of. Thus, convNN2 will not be considered when evaluating our hypothesis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ResNet34 and DenseNet121 vastly outperformed convNN2 and get significantly lower average mean error and standard deviation.\n",
    "Their errors on different datasets can be seen below\n",
    "##### ResNet34 Test Data\n",
    "![ResNet34 Test Data](jupyter_images/resnet34table.jpg) \n",
    "##### DenseNet121 Test Data\n",
    "![DenseNet121 Test Data](jupyter_images/densenet121table.jpg)\n",
    "\n",
    "From the tables we can see that ResNet had an error about 45% lower than DenseNet, and was a lot more sensitive to data bias from the data sets.\n",
    "The DenseNet had a higher error for women, regardless of dataset bias. The error on the Female test set was also very insensitive to dataset bias, indicating the model is not properly learning to predict the correct landmarks for women. \n",
    "\n",
    "\n",
    "If we look at a graph for both models to get a better understand of what is happening during training\n",
    "\n",
    "-- Add loss over time\n",
    "\n",
    "![Both Models Mean Error Change](jupyter_images/modelgraphs.jpg)\n",
    "\n",
    "ResNet mean error converges to a asymptote while DenseNet continues oscillating. This indicates that DenseNet had some issue during training and did not output a very optimal model. High oscillation tends to indicate that the learning rate is too high or that the training might benefit from using an optimiser with momentum. DenseNet is ostensibly a more sophisticated model as it has both more layers and parameters this did not appear to hold in practice. It appeared the model underfit to the female portion of the dataset while ResNet which is comparitively simple did. We theorise this is due to the DenseNet having vanishing gradient issues due to its high layer count. While DenseNet is intended to prevent vanishing gradients, the pytorch implementation only uses dense blocks. The layers inside these blocks possess dense connections but subsequent layers only receive inputs from the previous dense block's output layer. This saves on computational complexity by bringing down the number of weights but is not as effective at preventing vanishing gradients. The ResNet model however used fewer layers and took advantage of skip connections. Since it uses summation as opposed to concatonation to bypass layers, it is more computationally efficient. Due to this, the model does not need to be split into blocks and the outputs from earlier layers can reach all later layers. This theory would explain our concerns about vanishing gradients. Additionally, when we examined the model weights for our trained DenseNets, we found that many of the earlier weight parameters had changed by no more that 10^-4^, further supporting the vanishing gradient issues. This is a strong indicator but further testing would be required.\n",
    "\n",
    "-- Add diagrams of denseNet and resNet\n",
    "\n",
    "As discussed, research suggests that discrepencies between men and women in facial recognition is not due to dataset bias but differences in facial structure. Another difference between ResNet and denseNet is their choice of pooling algorithms. Max pooling is likely better at learning to find edges and contours in images as average pooling has a propensity to smooth values. If we assume, women tend to have softer features and therefor less defined contours on their face, it may be harder for a model using average pooling to identify these contours. Since men tend to have more defined features with more contrast, the model may cope better. This is reflected in our models as DenseNet performed worse than ResNet and Resnet features many more max pooling layers while densenet contains many average pooling layers.\n",
    "\n",
    "While underfitting can also be a symptom of poor hyperparameter selection or insufficient data, we do not believe this to be the case. The models were both trained on large datasets and we had spent some time testing  hyperparameters that would produce satisfactory results for all models.\n",
    "\n",
    "#### Further Discussion\n",
    "\n",
    "Clearly, the inherent differences between male and female facial structure can pose problems when developing facial recognition models. We have shown this is largely independent to any dataset bias. While we have identified the cause of these issues further work would be required to determine the extent of this bias. Since we have only tested on three landmarks, we are unable to determine if these issues effect all landmarks or if there is a small subset that are responsible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "* Investigating which landmarks most heavily influence gender bias in model predictions may be a good next step. It would help afirm our conclusions as if landmarks with less contrast were more biased, it would support our theory that it is the difference in facial definition causing the bias.\n",
    "\n",
    "* Examining different pooling algorithms would also help support our results. This could be done by training a version of denseNet with max pooling replacing average pooling layers and an average pool version of ResNet. If the denseNet performance improves while the resnet decreases, this would support the literature.\n",
    "\n",
    "* Testing different proportions of men and women in datasets would allow us to better understand how influenced the models are by bias and experiment with strategies and guidlines to make the models more resiliant to dataset bias. Due to time constraints this team only experimented with three different proportions due to time constraints but more datapoints would provide considerable more information.\n",
    "\n",
    "* Investigate the minimal number of facial features necessary to perform face recognition fairly.â€‹\n",
    "\n",
    "* This team would also like to suggest identifying how large of an impact vanishing gradients played on these results by performing the same tests with a smaller version of denseNet. If the scores increase or remain consistent that would point heavily to vanishing gradients as it indicates the model is over complicated.\n",
    "\n",
    "* This team is also interested on performing a similar experiment with different racial groups. Here we believe dataset bias plays a bigger role as datasets created in certain countries are likely to have the same distributions of ethnicities as their local populace. We also expect differences in skin tone and again, facial structure would also contribute. This experiment was cut for time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Our experiments thus far have provided promising insights into why facial recognition performs worse for certain groups, and more specifically genders. We were able to pinpoint differences in facial structure as a major contributing factor and some model design choices such as pooling algorithms that may exaserbate these issues. Additionally, we were able to find a model that was relatively unbiased whislt producing low errors. These findings have helped verify existing research and can help influence future facial recognition development to be fairer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
