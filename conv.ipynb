{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Convolutional NN\n",
    "\n",
    "The first NN used in our experiment is a basic NN featuring four convolutional layers, one linear layer and pooling and dropout layers.\n",
    "\n",
    "TODO: Include diagram here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNN2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 6)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc1(x) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Net\n",
    "\n",
    "This network is based on the pytorch implementation of [densenet121](https://pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html). A dense net was chosen as it is a common network used in image recognition and classification tasks.\n",
    "\n",
    "Since the default pytorch implementation produces an output tensor of shape [1000,] a few linear layers were added to turn this into a [6,] output in order to describe just three landmarks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(denseNN, self).__init__()\n",
    "        # Pytorch does not come with densenet121 installed and it must be downloaded.\n",
    "        # \n",
    "        self.dense121 = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "        self.fc1 = nn.Linear(1000, 600)\n",
    "        self.fc2 = nn.Linear(600, 100)\n",
    "        self.fc3 = nn.Linear(100, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense121(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process and Validation\n",
    "- include hyperparameters and issues with learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First some imports and class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        current_time = time.time()\n",
    "\n",
    "        duration = current_time - self.start_time\n",
    "    \n",
    "        hours = int(duration / 3600)\n",
    "        minutes = int((duration % 3600) / 60)\n",
    "        seconds = int((duration % 3600) % 60)\n",
    "\n",
    "        return f\"{hours}h {minutes}m {seconds}s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "The evaluate function is used for both validation and testing purposes. Here we calculate the straight line distance from each coordinate output by the model and the real label coordinates. \n",
    "\n",
    "Average distance was chosen over MSE or other similar error functions as it is easier to interpret.\n",
    "\n",
    "The function also produces the standard deviation of these scores as it may provide some useful information in terms of variance. A high variance would simply mean the model is simply guessing in a tight area and not producing unique coordinates for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_set_path, device):\n",
    "    UTKFace = CustomImageDataset(valid_set_path, 'UTKFace')\n",
    "    valid_set = DataLoader(UTKFace, \n",
    "                            500, \n",
    "                            shuffle=True)\n",
    "\n",
    "    # We're calculating the distance ourselves as using MSE loss doesnt \n",
    "    # allow us to square root terms individually.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _, landmarks in valid_set:\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            outputs = model(images).view([-1,3,2]) # organise into (x, y) pairs\n",
    "\n",
    "            land_idx = [8, 30, 39]  # The labels we are training for\n",
    "            difference = torch.square(outputs - landmarks[:, land_idx]).to(device)\n",
    "            difference = torch.sqrt(difference[:, 0] + difference[:, 1])\n",
    "\n",
    "    model.train()\n",
    "    return torch.mean(difference).item(), torch.std(difference).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for the training\n",
    "\n",
    "Here validation is only performed every 20 iterations as it adds a significant amount of time to the training process. Validating less frequently allows us to graph the results. Each time validation is performed we check the current set of weights against the previous best scoring model. If the current weights perform better we update the best model accordingly and record which iteration it occured in. \n",
    "\n",
    "SGD does not ensure the final epoch and iteration does not necessarily produce the best model. As such, validating regularly allows us to choose the best model. While we cannot do this every iteration due to the overhead, we assume validating regularly will yield a set of weights close to the optimal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, lr, device, valid_set, epochs=5):\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialise somewhere to save data for later graphing\n",
    "    batches = len(train_loader)\n",
    "    scores = np.empty([batches * epochs, 3])\n",
    "    scores[:] = np.nan\n",
    "\n",
    "    best_model = model\n",
    "    best_scores = {\"iteration\": 0, \n",
    "                \"mean\": 1000,\n",
    "                \"std\": 1000,\n",
    "                \"loss_list\": []}\n",
    "\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            images, _, _, _, landmarks = data   # images, age, gender, race, landmarks\n",
    "\n",
    "            # Zero paramter gradients\n",
    "            optimizer.zero_grad()\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            land_idx = [8, 30, 39]  # The indexs of the landmarks we are training with\n",
    "            loss = loss_func(outputs, landmarks[:, land_idx].view(-1, 6))\n",
    "            best_scores[\"loss_list\"].append(loss.item())    # Record for graphing later\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation is performed every 20 iterations due to its high overhead.\n",
    "            if i % 20 == 0:\n",
    "                mean, std = evaluate(model, valid_set, device)\n",
    "                scores[(epoch * batches) + i, 0] = (epoch * batches) + i\n",
    "                scores[(epoch * batches) + i, 1] = mean\n",
    "                scores[(epoch * batches) + i, 2] = std\n",
    "                print(f\"[{timer.elapsed_time()}] Epoch: {epoch}, iteration: {i}, loss: {loss.item()}, mean: {mean}, std: {std}\")\n",
    "\n",
    "                # If the current model is the best we have seen so far, preserver the weights\n",
    "                if mean < best_scores[\"mean\"]:\n",
    "                    best_model = copy.deepcopy(model)    # We need to copy to preserve weights\n",
    "                    best_scores[\"iteration\"] = (epoch * batches) + i\n",
    "                    best_scores[\"mean\"] = mean\n",
    "                    best_scores[\"std\"] = std\n",
    "            \n",
    "    # Remove iterations where we did not do any validation\n",
    "    filtered_scores = scores[~np.isnan(scores).any(axis=1)]\n",
    "\n",
    "    return best_model, best_scores, filtered_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used cli arguments to make the code in this notebook we can instead add the arguments to a string and pass the string to the `ArgumentParser` class.\n",
    "\n",
    "* -f, --train_file\n",
    "    - Used to specify the path to a training file. These should be text files with a list of images. For example `ll_training_75-25_split.txt` indicates the model should use the training set with the 75-25 gender split.\n",
    "* -vf, --validation_file\n",
    "    - Like the train file, this argument is used to indicate which subset of `UTKFace` to use for validation.\n",
    "* -b, --batch\n",
    "    - Set the batch size for training. All models were trained on a batch size of 32.\n",
    "* -m, --model\n",
    "    - Choose which model to train.\n",
    "    - The models availible are:\n",
    "        - \"convNN2\"\n",
    "        - \"dense\"\n",
    "        - \"resnet18\"\n",
    "        - \"resnet34\"\n",
    "        - \"resnet50\"\n",
    "* -lr, --learning_rate\n",
    "    - Specify the learning rate.\n",
    "* --cuda\n",
    "    - Including this argument will enable training on cuda.\n",
    "* -e, --epochs\n",
    "    - Specify the number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_str = \"train.py -b 32 -lr 0.0001 --cuda -f ll_training_75-25_split.txt -vf ll_validation_75-25_split.txt -e 50 -m dense\"\n",
    "\n",
    "# Read in args\n",
    "parser = ArgumentParser(arg_str)\n",
    "parser.add_argument(\"-f\", \"--train_file\",\n",
    "                    help=\"Path to data file.\", \n",
    "                    metavar=\"FILE_PATH\", \n",
    "                    default=\"landmark_list.txt\")\n",
    "parser.add_argument(\"-vf\", \"--validation_file\",\n",
    "                    help=\"Choose file to use for validation.\",\n",
    "                    metavar=\"FILE_PATH\",\n",
    "                    default=\"landmark_list.txt\")\n",
    "parser.add_argument(\"-b\", \"--batch\", \n",
    "                    help=\"Batch size for training.\", \n",
    "                    type=int, \n",
    "                    metavar=\"INT\",\n",
    "                    default=64)\n",
    "parser.add_argument(\"-m\", \"--model\",\n",
    "                    help=\"Choose which model structure to use.\",\n",
    "                    default=\"convNN2\",\n",
    "                    metavar=\"MODEL_NAME\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\",\n",
    "                    help=\"Learning rate to run the optimizer function with.\",\n",
    "                    default=0.0001,\n",
    "                    type=float,\n",
    "                    metavar=\"FLOAT\")\n",
    "parser.add_argument(\"--cuda\",\n",
    "                    help=\"Add this argument to run the code using GPU acceleration.\",\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument(\"-e\", \"--epochs\",\n",
    "                    help=\"Dictate number of epochs to train for.\",\n",
    "                    type=int,\n",
    "                    metavar=\"INT\",\n",
    "                    default=5)\n",
    "\n",
    "args, _  = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomImageDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mmodel \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdense\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     model \u001b[39m=\u001b[39m denseNN(device)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m UTKFace \u001b[39m=\u001b[39m CustomImageDataset(args\u001b[39m.\u001b[39mtrain_file, \u001b[39m'\u001b[39m\u001b[39mUTKFace\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(UTKFace, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m                                 batch_size\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mbatch, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/conv.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m                                 shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomImageDataset' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if args.cuda and torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "\n",
    "model = None\n",
    "if args.model == \"convNN2\":\n",
    "    model = convNN2().to(device)\n",
    "elif args.model == \"resnet18\":\n",
    "    model = resnet18().to(device)\n",
    "elif args.model == \"resnet34\":\n",
    "    model = resnet34().to(device)\n",
    "elif args.model == \"resnet50\":\n",
    "    model = resnet50().to(device)\n",
    "elif args.model == \"dense\":\n",
    "    model = denseNN(device).to(device)\n",
    "\n",
    "UTKFace = CustomImageDataset(args.train_file, 'UTKFace')\n",
    "train_dataloader = DataLoader(UTKFace, \n",
    "                                batch_size=args.batch, \n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training {args.model} from {args.train_file} with batch_size={args.batch}\\n\")\n",
    "\n",
    "# Train model\n",
    "model, info, plots = train(model, train_dataloader, args.learning_rate, device, args.validation_file, epochs=args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training/validation results\n",
    "# filename includes batchsize, epoch number, learning rate\n",
    "filename = f\"{args.model}_{args.train_file}_batch{args.batch}_ep{args.epochs}_lr{args.learning_rate}\"\n",
    "model_path = f\"./models/{filename}.pt\"\n",
    "scores_path = f\"./model_scores/{filename}.csv\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "np.savetxt(scores_path, plots, delimiter=\",\")\n",
    "\n",
    "info[\"epochs\"] = args.epochs\n",
    "info[\"batch\"] = args.batch\n",
    "\n",
    "with open(f\"./model_infos/{filename}.json\", \"w\") as outfile:\n",
    "    json.dump(info, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "marcos stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
