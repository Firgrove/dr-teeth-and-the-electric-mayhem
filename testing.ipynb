{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network\n",
    "\n",
    "We can commence testing now that all the networks have been successfully trained. The models and training data are in the folders: *models*, *model_scores*, and *model_infos*.\n",
    "Each model will be tested and compared on three different sets: a 50-50 split of male to female testing data, a solely male testing set, and a solely female testing set.\n",
    "\n",
    "First we will load all the models into memory. This is simply done by insert them all into a list, initialising a model, and then loading each file into that subsequent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convNN2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31990/2567170541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \"resnet34_ll_training_75-25_split.txt_batch64_ep50_lr0.0001.pt\"]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenseNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenseNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenseNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convNN2' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Adding in model names\n",
    "model_names = [\"convNN2_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"dense_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\", \"resnet34_ll_training_50-50_split.txt_batch64_ep50_lr0.0001.pt\", \"resnet34_ll_training_25-75_split.txt_batch64_ep50_lr0.0001.pt\",\n",
    "        \"resnet34_ll_training_75-25_split.txt_batch64_ep50_lr0.0001.pt\"]\n",
    "\n",
    "models = [convNN2(), convNN2(), convNN2(), denseNN(), denseNN(), denseNN(), resnet34(), resnet34(), resnet34()]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    model.load_state_dict(torch.load(\"./models/\" + name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Once we have done this we can now calculate the scores of each model for every test set. This should compute the mean error and standard deviation and return it as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    print(\"50-50 test file \" + evaluate(model, \"./ll_test_50-50\", \"cpu\"))\n",
    "    print(\"male test file \" + evaluate(model, \"./ll_test_males\", \"cpu\"))\n",
    "    print(\"female test file \" + evaluate(model, \"./ll_test_females\", \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its simplicity, the error for the convNN2 is much higher than the the other two models. \n",
    "While we can see that ResNet34 outperformed DesneNet121 with a significant reduction in error between the two, \n",
    "both models had higher error in females for identifing landmarks for both genders, as expected at the start of this project.\n",
    "\n",
    "Both models performed best overall on balanced data sets, though this difference was very slight. It also shows that a higher proportional of females or males in the dataset does not neccesarily result in a lower error for each respective gender, but the change in error was quite minimal, meaning we can discard the bias in the dataset as a source of the bias between males and females.\n",
    "\n",
    "Now we can generate graphs for the change in mean error and standard deviation during training, and how it progressed over the iterations.\n",
    "When loading in the model scores file, we need to parse this data to make it useful to us. Namely, we split it into a list by using ',' as dividers.\n",
    "The model infos are already presented in json format, and can simply be read in directly. This gives us the iteration, mean, and standard deviation of the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in models:\n",
    "    epoch = []\n",
    "    error = []\n",
    "    std = []\n",
    "    m_nopt = args.model.split(\".pt\")\n",
    "    with open(\"./model_scores/\" + m_nopt[0] + \".csv\") as file:\n",
    "        for line in file:\n",
    "            scores = line.split(\",\")   \n",
    "            epoch.append(float(scores[0]))\n",
    "            error.append(float(scores[1]))\n",
    "            std.append(float(scores[2]))\n",
    "    \n",
    "    file = open(\"./model_infos/\" + m_nopt[0] + \".json\")\n",
    "    model_info = json.load(file)\n",
    "\n",
    "    plt.plot(epoch, error)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Error\")\n",
    "    plt.title('Error change during training')\n",
    "\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"mean\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch, std)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Standard Deviation\")\n",
    "    plt.title('STD change during training')\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"std\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we can see that the mean error and standard deviation rapidly decreases over time within the first few iterations. Initially, with the weights initialized to zero, the predicted landmarks all fall very far from the correct points.\n",
    "\n",
    "Both the ConvNN2 and DenseNet seem to oscillate between certain points and do not converge to a single point like the Resnet34 does. This seems to indicate that they are not necessarily the most optimal model.\n",
    "\n",
    "We can also see the loss during training with the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph for loss during training\n",
    "for model in models:\n",
    "    plt.plot(list(range(len(model_info[\"loss_list\"]))), model_info[\"loss_list\"])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title('Loss during training')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually check how accurate the predicted landmarks are on an image by comparing them against the input landmarks used by the model for training. For each model an image will be generated that plots 2 lines, with 5 images per line for easy comparison between different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(train_dataloader, axs_flat):\n",
    "    with torch.no_grad():\n",
    "            for i, (image, _, _, _, labels) in enumerate(train_dataloader):\n",
    "                output = model(image)\n",
    "                output = output.reshape(3,2)\n",
    "                image = image.squeeze()\n",
    "                image = image.permute(1, 2, 0)    #Default was 3,200,200\n",
    "                im = axs_flat[i].imshow(image)\n",
    "                x = np.array(range(200))\n",
    "\n",
    "                # Finding landmarks for chin, nose and eye this txt file for each image\n",
    "                land_idx = [8, 30, 39]\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels[land_idx, :]\n",
    "                #ax.scatter(output[:,0], output[:,1], linewidth=2, color='red')\n",
    "                axs_flat[i].scatter(output[:,0], output[:,1], linewidth=2, color='c', s = 5)\n",
    "                axs_flat[i].scatter(labels[:,0], labels[:,1], linewidth=2, color='m', s = 5)\n",
    "\n",
    "UTKFace = CustomImageDataset(\"./test_output_images\", 'UTKFace')\n",
    "train_dataloader = DataLoader(UTKFace, \n",
    "                                    batch_size=1, \n",
    "                                    shuffle=False)\n",
    "\n",
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    fig, axs = plt.subplots(2,5, figsize=(20,10))\n",
    "    axs_flat = axs.flatten()\n",
    "\n",
    "    generate_images(train_dataloader, axs_flat)\n",
    "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.legend(('Predicted output','Expected output'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "As we can see the basic convolutional neural network essentially predicts the same three points for every single image. While that does produce the best possible mean error for this specific model, the results do show that the model is severely underfitting. It does not have enough layers, depth, or complexity to recognize the different landmark features on each face.\n",
    "\n",
    "Currently, the model produces a very high mean error (54.377 and 54.895 pixels for females and males, respectively). On a 200x200 pixel image this is very far off. Thus, convNN2 will not be considered when evaluating our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ResNet34 and DenseNet121 vastly outperformed convNN2 and get significantly lower average mean error and standard deviation.\n",
    "Their errors on different datasets can be seen below\n",
    "##### ResNet34 Test Data\n",
    "![ResNet34 Test Data](jupyter_images/resnet34table.jpg) \n",
    "##### DenseNet121 Test Data\n",
    "![DenseNet121 Test Data](jupyter_images/densenet121table.jpg)\n",
    "\n",
    "From the tables we can see that ResNet had a mean error about 45% lower than DenseNet, which was a lot more sensitive to data bias from the data sets.\n",
    "The DenseNet had a higher error for women regardless of dataset bias. The error on the female test set was also very insensitive to dataset bias, indicating that the model is not properly learning how to predict the correct landmarks for women.\n",
    "\n",
    "We can look at a graph for all three models to get a better understand of what is happening during training.\n",
    "\n",
    "##### convNN2 Loss\n",
    "![convNN2 loss](jupyter_images/lossconnv2.jpg)\n",
    "##### DenseNet121 Loss\n",
    "![densenet121 loss](jupyter_images/lossdense.jpg)\n",
    "##### ResNet34 Loss\n",
    "![resnet34 loss](jupyter_images/lossresnet.jpg)\n",
    "\n",
    "\n",
    "Another issue may be due to underfitting of the model, which may be experimented with by increasing the amount of layers in the network in the future. If there is little to no increase in mean error, it would highlight that vanishing gradients are at play and that the number of layers must be reduced instead.\n",
    "\n",
    "![Both Models Mean Error Change](jupyter_images/modelgraphs.jpg)\n",
    "\n",
    "ResNet mean error converges asymptotically, while DenseNet continues oscillating. This indicates that the DenseNet model had issues during training and did not ouput a very optimal model. High oscillation tends to indicate that the learning rate is too high, or that the training might benefit from using an optimiser with momentum. DenseNet is ostensibly a more sophisticated model as it has both more layers and parameters, but this did not appear to hold in practice. It appeared the model was unable to properly fit to the female portion of the dataset, while ResNet despite being comparatively simpler, was able to. We theorise this is due to the DenseNet having vanishing gradient issues due to its high layer count. While DenseNet is intended to prevent vanishing gradients, the pytorch implementation only uses dense blocks. The layers inside these blocks possess dense connections, but subsequent layers only receive inputs from the previous dense block's output layer. This saves on computational complexity by bringing down the number of weights. However, this could allow vanishing gradients to exist. The ResNet model however used fewer layers and took advantage of skip connections. Since it uses summation as opposed to concatenation to bypass layers, it is more computationally efficient. Due to this, the model does not need to be split into blocks and the outputs from earlier layers can reach all later layers. This theory would explain our concerns about vanishing gradients. Additionally, when we examined the model weights for our trained DenseNets, we found that many of the earlier weight parameters had changed by no more that 1e-4, further supporting the vanishing gradient issues.\n",
    "\n",
    "As discussed, research suggests that discrepencies between men and women in facial recognition are not due to dataset bias but rather differences in facial structure. Another difference between ResNet and denseNet is their choice of pooling algorithms. Max pooling is likely better at learning to find edges and contours, while average pooling has a propensity to smooth values. If we assume that women tend to have softer features and therefore less defined contours on their face, it may be harder for a model using average pooling to identify these contours. Since men tend to have more defined features with more contrast, the model may cope better. This is reflected in our models as DenseNet performed worse than ResNet and Resnet features many more max pooling layers while densenet contains many average pooling layers.\n",
    "\n",
    "While underfitting can also be a symptom of poor hyperparameter selection or insufficient data, we do not believe this to be the case. The models were both trained on large datasets and we had spent some time testing  hyperparameters that would produce satisfactory results for all models.\n",
    "\n",
    "#### Further Discussion\n",
    "\n",
    "Clearly, the inherent differences between male and female facial structure can pose problems when developing facial recognition models. We have shown this is largely independent of any dataset bias. While we have identified the cause of these issues, further work would be required to determine the extent of this bias. Since we have only tested on three landmarks, we are unable to determine if these issues affect all landmarks or if there is a small subset that are responsible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "* Investigating which landmarks most heavily influence gender bias in model predictions may be a good next step. It would help affirm our conclusions if landmarks with less contrast were more biased, as it would support our theory that it is the difference in facial definition causing the bias.\n",
    "\n",
    "* Examining different pooling algorithms would also help support our results. This could be done by training a version of denseNet with max pooling instead of average pooling between each dense block, and a version of Resnet with without max pooling. If the denseNet performance improves while the resnet decreases, this would support the literature.\n",
    "\n",
    "* Testing different proportions of men and women in datasets would allow us to better understand how influenced the models are by bias and experiment with strategies and guidelines to make the models more resiliant to dataset bias. Due to time constraints this team only experimented with three different proportions, but more datapoints would provide considerably more information.\n",
    "\n",
    "* Investigate the minimal number of facial features necessary to perform face recognition fairly.​\n",
    "\n",
    "* This team would also like to suggest identifying how large of an impact vanishing gradients played on these results by performing the same tests with a smaller version of denseNet. If the scores increase or remain consistent that would point heavily to vanishing gradients as it indicates the model is overly complex.\n",
    "\n",
    "* This team is also interested on performing a similar experiment with different racial groups. Here we believe dataset bias plays a bigger role as datasets created in certain countries are likely to have the same distributions of ethnicities as their local populace. We also expect differences in skin tone and again, facial structure would also contribute. This experiment was cut for time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Our experiments thus far have provided promising insights into why facial recognition performs worse for certain groups, and more specifically genders. We were able to pinpoint differences in facial structure as a major contributing factor and some model design choices such as pooling algorithms that may exacerbate these issues. Additionally, we were able to find a model that was relatively unbiased while still producing low errors. These findings have helped verify existing research and can help influence future facial recognition development to be fairer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd3680c169485ba1b9a9722bd93146304e6d434af8a1dd1496d4e3ce75ad2fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
