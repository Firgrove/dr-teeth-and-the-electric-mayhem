{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network\n",
    "\n",
    "Now that all the networks have been successfully trained, their models and training data in folders models, model_scores and model_infos we can commence testing.\n",
    "Each model will be tested on a 50-50 split of male to female testing data, soely male testing set and soely female testing set to compare between each other.\n",
    "\n",
    "First we will load all the models into memory, this is simply done by insert them all into a list and then initialising a model and then loading each file into that subsequent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Adding in model names\n",
    "model_names = [\"convNN2_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"convNN2_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"convNN2_ll_training_27-75_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"dense_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"dense_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\", \"resnet34_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \"resnet34_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"resnet34_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\"]\n",
    "\n",
    "models = [convNN2(), convNN2(), convNN2(), denseNN(), denseNN(), denseNN(), resnet34(), resnet34(), resnet34()]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    model.load_state_dict(torch.load(\"./models/\" + name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Once we have done this we can now calculate the scores of each model on each test set, this should compute the mean error and std and return it as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    print(\"50-50 test file \" + evaluate(model, \"./ll_test_50-50\", \"cpu\"))\n",
    "    print(\"male test file \" + evaluate(model, \"./ll_test_males\", \"cpu\"))\n",
    "    print(\"female test file \" + evaluate(model, \"./ll_test_females\", \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error for the convNN2 is much higher than the the other two models, this is due to the simplicity of the model. \n",
    "While we can see that ResNet34 outperformed DesneNet121 with a significant reduction in error between the two.\n",
    "Both models had higher error in females for identifing landmarks for both genders, as expected started this project.\n",
    "\n",
    "Both models performed best overall on balanced data sets, though this difference was very slight.\n",
    "It also shows that a higher proportional of females or males in the dataset does not neccesarily result in a lower error for each respective gender, but the change in error was quite minimal \n",
    "meaning we can discard the bias in the dataset as a source of the bias between males and females.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate graphs for the change in mean error and standard deviation during training, and how it progressed over the iterations.\n",
    "\n",
    "All the extra saved data, when the model is loaded in and plotted on two seperate graphs.\n",
    "When loading in the model scores file, we need to parse this data to make it useful to us and is split into a list by using ',' as dividers.\n",
    "While the model infos are already presented in a json file and ready to read in simply, this gives us the iteration and mean or std of the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in models:\n",
    "    epoch = []\n",
    "    error = []\n",
    "    std = []\n",
    "    m_nopt = args.model.split(\".pt\")\n",
    "    with open(\"./model_scores/\" + m_nopt[0] + \".csv\") as file:\n",
    "        for line in file:\n",
    "            scores = line.split(\",\")   \n",
    "            epoch.append(float(scores[0]))\n",
    "            error.append(float(scores[1]))\n",
    "            std.append(float(scores[2]))\n",
    "    \n",
    "    file = open(\"./model_infos/\" + m_nopt[0] + \".json\")\n",
    "    model_info = json.load(file)\n",
    "\n",
    "    plt.plot(epoch, error)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Error\")\n",
    "    plt.title('Error change during training')\n",
    "\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"mean\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch, std)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Standard Deviation\")\n",
    "    plt.title('STD change during training')\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"std\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we can see that all error all std over time rapidly decreases within the first few iterations as the weights start off as zero, and the furthest possible away from the correct points (chin, nose and left eye) as possible.\n",
    "Both the ConvNN2 and DenseNet seem to oscillate between certain points and do not converge to a single point like the Resnet34 does, this seems to indicate that they have not obtained the most optimal model.\n",
    "\n",
    "We can visually check how accurate and close the predictions of the landmarks are on an image, comparing these against the input landmarks used by the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_images(train_dataloader, axs_flat):\n",
    "    with torch.no_grad():\n",
    "            for i, (image, _, _, _, labels) in enumerate(train_dataloader):\n",
    "                output = model(image)\n",
    "                output = output.reshape(3,2)\n",
    "                image = image.squeeze()\n",
    "                image = image.permute(1, 2, 0)    #Default was 3,200,200\n",
    "                im = axs_flat[i].imshow(image)\n",
    "                x = np.array(range(200))\n",
    "\n",
    "                # Finding landmarks for chin, nose and eye this txt file for each image\n",
    "                land_idx = [8, 30, 39]\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels[land_idx, :]\n",
    "                #ax.scatter(output[:,0], output[:,1], linewidth=2, color='red')\n",
    "                axs_flat[i].scatter(output[:,0], output[:,1], linewidth=2, color='c', s = 5)\n",
    "                axs_flat[i].scatter(labels[:,0], labels[:,1], linewidth=2, color='m', s = 5)\n",
    "\n",
    "UTKFace = CustomImageDataset(\"./test_output_images\", 'UTKFace')\n",
    "        train_dataloader = DataLoader(UTKFace, \n",
    "                                        batch_size=1, \n",
    "                                        shuffle=False)\n",
    "\n",
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    fig, axs = plt.subplots(2,5, figsize=(20,10))\n",
    "    axs_flat = axs.flatten()\n",
    "\n",
    "    generate_images(train_dataloader, axs_flat)\n",
    "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.legend(('Predicted output','Expected output'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "As we can see the basic convolutional neural network essentially predicts the same 3 points for every single image, since that gives the best average mean error for the model\n",
    "as the model is underfitting. Since it doesn't have enough layers, depth or complexity in order to compute, recognise and plots the landmarks on different features of the face. \n",
    "\n",
    "Currently the model produces a very high mean error 54.377 and 54.895 for females and males, and in a 200x200 image this is very far of. \n",
    "\n",
    "\n",
    "There is not much that can be done to improve on since the error may reduce slightly as more layers are added, but since it is a basic model the error would start going back soon after adding more layers\n",
    "due to the problem of vanishing gradient and potentially overfitting of the model. This would be a problem as when calculating the product of the derivative for the weights during backpropogation\n",
    "decreases to zero until the partial derivative of the loss function approaches zero and the partial derivative disappears.\n",
    "\n",
    "One of the main reason resnet and densenet were developed was to attempt to solve this vanishing gradient issue in neural networks, and as such will be more appropriate in this example.\n",
    "Thus, convNN2 will not be considered when evaluating our hypothesis.\n",
    "\n",
    "This basic neural net was useful in testing the limits of the possible calculations for a neural network in performing a more specialised task, additionally it forced us to explore other\n",
    "neural network models that were more useful. Additionally, it gave us an insight into the amount of work and research done into various nets and trying to improve them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ResNet34 and DenseNet121 vastly outperformed convNN2 and get significantly lower average mean error and standard deviation.\n",
    "Their errors on different datasets can be seen below\n",
    "##### ResNet34 Test Data\n",
    "![ResNet34 Test Data](jupyter_images/resnet34table.jpg) \n",
    "##### DenseNet121 Test Data\n",
    "![DenseNet121 Test Data](jupyter_images/densenet121table.jpg)\n",
    "\n",
    "From the tables we can see that ResNet had an error about 45% lower than DenseNet, and was a lot more sensitive to data bias from the data sets.\n",
    "The DenseNet has regardless of data set bias higher mean error for women than men, while for ResNet this error flipped with datasets that utilised a larger subset of women than men.\n",
    "\n",
    "If we look at a graph for \n",
    "![Both Models Mean Error Change](jupyter_images/modelgraphs.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not uniquely data issue - issues fitting model & disprepancy m & f, across all training dataset regardless of bias\n",
    "- densenet normally used image classification, issue in comonly used nn - not the root cause\n",
    "densenet underfitting - sensitive enough to facial structure/subultely for women - vanishing gradient (model not complex enough)\n",
    "\n",
    "-need more complicated & sensitive model\n",
    "\n",
    "-resnet fit well more affected by dataset bias - fair consisntel\n",
    "-skip connections cant travel right from start to end like densenet (denseblocks)\n",
    "\n",
    "\n",
    "#### Future Work\n",
    "-main focus differences in accuracy betweeen m & f \n",
    "-- hyperparametres not fine tuned - can be improved in future (difference in bias, rather than relative performacne)\n",
    "\n",
    "-adding more layers to densenet\n",
    "-reduce layers\n",
    "-try out different pooling algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
