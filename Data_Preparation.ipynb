{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of Biased Gender Data on Face Recognition\n",
    "* **Course:** Neural Networks and Deep Learning (COMP9444) 22T3\n",
    "* **Mentor:** Kunzie Xie\n",
    "* **Team:** Dr. Teeth and the Electric Mayhem\n",
    "* **Members:**\n",
    "    * _Daniel Gotilla_ (z5343046@student.unsw.edu.au)\n",
    "    * _John Conlon_ (z5257381@student.unsw.edu.au)\n",
    "    * _John Pham_ (z3216645@student.unsw.edu.au)\n",
    "    * _Marco Seidenberg_ (z5264260@student.unsw.edu.au)\n",
    "    * _Oscar Feng_ (z5396050@student.unsw.edu.au)\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Downloading, Concatenating, Validating and Correcting the Original Data\n",
    "\n",
    "We chose to leverage the [UTKFace](https://github.com/aicip/UTKFace) dataset, built by Yang Song and Zhifei Zhang at the [Advanced Imaging and Collaborative Information Processing (AICIP) Lab](https://aicip.github.io/) at [The University of Tennessee, Knoxville (UTK)](http://www.utk.edu/). This dataset contains over 28k face images with annotations for age, gender, and ethnicity and facial landmark coordinates and is available for non-commercial, research purposes.\n",
    "\n",
    "The dataset is provided in separate files (through links to Google Drive):\n",
    "1. **[In-the-wild Faces (1.3GB)](https://drive.google.com/open?id=0BxYys69jI14kSVdWWllDMWhnN2c)** — original images, containing one or more faces. _Not used in our study._\n",
    "2. **[Aligned & Cropped Faces (107MB)](https://drive.google.com/drive/folders/0BxYys69jI14kU0I1YUQyY1ZDRUE?usp=sharing)** — above images, cropped tightly around the faces of each subject; filenames contain the age, gender, race and datetime metadata.\n",
    "3. **[Landmarks (68 points, 12MB)](https://drive.google.com/open?id=0BxYys69jI14kS1lmbW1jbkFHaW8)** — 3 separate text files listing all the aligned/cropped faces (in 2, above) and their associated facial landmarks.\n",
    "\n",
    "Firstly, we need to download the 3 landmark list files from the [UTKFace Google Drive](https://drive.google.com/open?id=0BxYys69jI14kS1lmbW1jbkFHaW8) and copy them to the project directory (without renaming); you will have three files named:\n",
    "* `landmark_list_part1.txt` (9,780 lines)\n",
    "* `landmark_list_part2.txt` (10,719 lines)\n",
    "* `landmark_list_part3.txt` (3,209 lines)\n",
    "\n",
    "We can use the `concatenate_files` function (below) to join the 3 landmark list files into a single file called `landmark_list_concatenated.txt`, deleting the 3 original files in the process. (This function will be used extensively below to consolidate the training, validation and testing datasets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './landmark_list_part1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Concatenating the 3 landmark files into a single one\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m ll_parts \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m./landmark_list_part1.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m./landmark_list_part2.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m./landmark_list_part3.txt\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m concatenate_files(\u001b[39m\"\u001b[39;49m\u001b[39mlandmark_list_concatenated.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, files\u001b[39m=\u001b[39;49mll_parts, delete\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 2\u001b[0m in \u001b[0;36mconcatenate_files\u001b[0;34m(target, files, delete, randomise, randomseed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m lines \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m files:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(name, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             lines\u001b[39m.\u001b[39mappend(line)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './landmark_list_part1.txt'"
     ]
    }
   ],
   "source": [
    "from os import remove\n",
    "from random import shuffle, seed\n",
    "\n",
    "def concatenate_files(target: str, files: list, *, delete: bool = False, randomise: bool = False,\n",
    "                      randomseed=None):\n",
    "    \"\"\"\n",
    "    Concatenates multiple files, producing a target file and (optionally) randomising lines and/or deleting the input files.\n",
    "\n",
    "    :param target: Name for new (output) file concatenating the input files.\n",
    "    :param files: List of files to concatenate.\n",
    "    :param delete: Should the input files be deleted after concatenation?\n",
    "    :param randomise: Should the lines of the input files be randomised?\n",
    "    :param randomseed: If so, should we use a specific seed value?\n",
    "    :return: No return value but a file with the target name will be created in the current directory.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    for name in files:\n",
    "        with open(name, 'r') as f:\n",
    "            for line in f:\n",
    "                lines.append(line)\n",
    "\n",
    "    if randomise:\n",
    "        if seed is not None:\n",
    "            seed(randomseed)\n",
    "        shuffle(lines)\n",
    "\n",
    "    with open(target, \"w\") as new_file:\n",
    "        for line in lines:\n",
    "            new_file.write(line)\n",
    "\n",
    "    if delete:\n",
    "        for name in files:\n",
    "            remove(name)\n",
    "\n",
    "    print(f\"Input files successfully concatenated as\", target)\n",
    "\n",
    "# Concatenating the 3 landmark files into a single one\n",
    "ll_parts = [\"landmark_list_part1.txt\", \"landmark_list_part2.txt\", \"landmark_list_part3.txt\"]\n",
    "concatenate_files(\"landmark_list_concatenated.txt\", files=ll_parts, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The three partial files were substituted by a single, consolidated file named `landmark_list_concatenated.txt` with 23,708 lines, one for each cropped image in the dataset.\n",
    "\n",
    "If you open and examine the file, you'll see something like this:\n",
    "```\n",
    "1_0_2_20161219140530307.jpg -4 71 -4 96 -3 120 -1 144 9 166 28 179 53 186 77 192 100 194 121 191 142 183 161 174 180 161 192 142 195 120 194 97 192 74 16 53 29 39 48 33 68 34 86 40 113 39 129 33 148 32 164 37 175 49 100 59 101 72 101 85 101 99 78 112 89 113 100 116 110 114 120 111 39 62 51 61 61 60 71 65 60 63 50 62 124 64 134 59 144 59 155 62 144 62 134 62 55 137 72 134 87 132 97 133 107 131 120 132 136 133 121 143 109 146 98 147 88 146 72 145 61 138 87 137 97 138 107 136 130 135 108 139 98 140 88 139\n",
    "```\n",
    "\n",
    "Each line follows the pattern `A_G_E_DDDDDDDDTTTTTTTTT.jpg X1 Y1 X2 Y2 … X68 Y68` where:\n",
    "* `A` stands for the subject's age in years (1 to 3 digits);\n",
    "* `G` stands for the subject's gender, where\n",
    "    * 0 is male and\n",
    "    * 1 is female;\n",
    "* `E` stands for the subject's (perceived) ethnicity, where\n",
    "    * 0 is \"white\",\n",
    "    * 1 is \"black\",\n",
    "    * 2 is \"asian\",\n",
    "    * 3 is \"indian\",\n",
    "    * 4 is \"other\";\n",
    "* `DDDDDDDDTTTTTTTTT` stands for the date/time the image was added to the dataset;\n",
    "* `X1 Y1 X2 Y2 … X68 Y68` stands for the 68 (x,y) coordinate pairs for the facial landamarks where\n",
    "    * Pairs 1 through 17 map the contour of the subject's face, from ear to ear around chin;\n",
    "    * Pairs 18 through 22 map the contour of the subject's left eyebrow;\n",
    "    * Pairs 23 through 27 map the contour of the subject's right eyebrow;\n",
    "    * Pairs 28 through 31 map the contour of the line on top of the subject's nose;\n",
    "    * Pairs 32 through 36 map the contour of the bottom part of the subject's nose;\n",
    "    * Pairs 37 through 42 map the contour of the subject's left eye;\n",
    "    * Pairs 43 through 48 map the contour of the subject's right eye;\n",
    "    * Pairs 49 through 60 map the outer contour of the subject's lips;\n",
    "    * Pairs 61 through 68 map the inner contour of the subject's lips;\n",
    "\n",
    "Now let us download the \"Aligned & Cropped Faces\" images from UTKFace's [Google Drive](https://drive.google.com/drive/folders/0BxYys69jI14kU0I1YUQyY1ZDRUE?resourcekey=0-01Pth1hq20K4kuGVkp3oBw)  (you want \"UTKFace.tar.gz\" which sits at about 102MB). Copy the _UTKFace.tar.gz_ file into the project directory and unzip it; you should have a folder named `UTKFace` with 23,708 files. It is a good sign that the number of image files matches the number of lines in `landmark_list_concatenated.txt`, but let us double-check if all the images in the folder are listed in the landmark file and vice-versa. Note that the images are listed in the landmark file with the `.jpg` extension, but the actual image files in the `UTKFace` directory use the `.jpg.chip.jpg` extension. The following script accounts for that when validating the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'landmark_list_concatenated.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m probs \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# issues found\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m matches \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# matching files found\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(landmarks_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     lines \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m images \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.chip.jpg\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'landmark_list_concatenated.txt'"
     ]
    }
   ],
   "source": [
    "from os import listdir, path\n",
    "\n",
    "directory = 'UTKFace'  # directory with images\n",
    "landmarks_file = 'landmark_list_concatenated.txt'  # list of images with landmarks\n",
    "probs = 0  # issues found\n",
    "matches = 0  # matching files found\n",
    "\n",
    "with open(landmarks_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "images = [line.split()[0] + \".chip.jpg\" for line in lines]\n",
    "image_set = set()\n",
    "for image in images:\n",
    "    file = path.join(directory, image)\n",
    "    if path.exists(file):\n",
    "        image_set.add(image)\n",
    "    else:\n",
    "        print(\"Missing image:\", image)\n",
    "        probs += 1\n",
    "\n",
    "# iterate over files in that directory\n",
    "for filename in listdir(directory):\n",
    "    f = path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if filename not in image_set:\n",
    "        print(\"Unlisted image:\", filename)\n",
    "        probs += 1\n",
    "    else:\n",
    "        splitname: list[str] = filename.split(\".\")[0].split(\"_\")\n",
    "        if len(splitname) != 4 or int(splitname[0]) < 1 or int(splitname[0]) > 200 or int(splitname[1]) > 1 or int(splitname[2]) > 4 or not splitname[3].isdigit():\n",
    "            print(\"Misnamed image:\", filename)\n",
    "            probs += 1\n",
    "        else:\n",
    "            matches += 1\n",
    "print(\"\\nIssues found:\", probs)\n",
    "print(\"Matching images:\", matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can see that most of the listed images are missing metadata fields or have minor typos such as missing a period/character or an extraneous space character. And when that is the case, the image is listed twice: once as a \"missing image\" and once as an \"unlisted image\". The following script will apply the corrections to the landmark file, yielding a new file named `landmark_list_corrected.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find 'landmark_list_concatenated.txt'\n"
     ]
    }
   ],
   "source": [
    "if path.exists('landmark_list_concatenated.txt'):\n",
    "\n",
    "    with open('landmark_list_concatenated.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Correcting specific lines of the spreadsheet so they match the actual image names\n",
    "    lines[8512] = lines[8512].replace(\"61_1_20170109142408075.jpg\", \"61_1_1_20170109142408075.jpg\")  # Missing gender\n",
    "    lines[8513] = lines[8513].replace(\"61_3_20170109150557335.jpg\", \"61_1_3_20170109150557335.jpg\")  # Missing gender\n",
    "    lines[13951] = lines[13951].replace(\"53__0_20170116184028385.jpg\", \"53_1_0_20170116184028385.jpg\")  # Missing gender\n",
    "    lines[20080] = lines[20080].replace(\"39_1_20170116174525125.jpg\", \"39_1_1_20170116174525125.jpg\")  # Missing gender\n",
    "    lines[20585] = lines[20585].replace(\"55_0_0_20170116232725357jpg\", \"55_0_0_20170116232725357.jpg\")  # Missing period\n",
    "    lines[20621] = lines[20621].replace(\"24_0_1_20170116220224657 .jpg\", \"24_0_1_20170116220224657.jpg\")  # Space in name\n",
    "    lines[20647] = lines[20647].replace(\"44_1_4_20170116235150272.pg\", \"44_1_4_20170116235150272.jpg\")  # Wrong extension\n",
    "\n",
    "    with open('landmark_list_corrected.txt', \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "    print(\"Corrections successfully applied to 'landmark_list_corrected.txt'\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"ERROR: Could not find 'landmark_list_concatenated.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, we still need to correct 5 issues in image filenames; if using an Unix-compatible system (such as Linux, Mac or Windows with WSL), you can run the following shell script:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env sh\n",
    "\n",
    "UTKZIP='UTKFace.tar.gz';\n",
    "UTKDIR='UTKFace';\n",
    "\n",
    "\tif [ ! -d \"$UTKDIR\" ]; then\n",
    "\t\tif [ -f \"$UTKZIP\" ]; then\n",
    "\t\t  echo \"Unzipping $UTKZIP to $UTKDIR...\"\n",
    "\t\t\ttar -xvf UTKFace.tar.gz 2>/dev/null\n",
    "\t\t\techo \"Done. Feel free to delete $UTKZIP.\"\n",
    "\t\telse\n",
    "\t\t  echo \"$UTKDIR directory not found. Please download $UTKZIP from https://gotil.la/3ziDcCX\"\n",
    "\t\tfi\n",
    "\tfi\n",
    "\tif [ -d \"$UTKDIR\" ]; then\n",
    "\t  echo \"$UTKDIR directory found.\"\n",
    "\t  cnt=0\n",
    "\t\tif [ -f \"$UTKDIR/24_0_1_20170116220224657 .jpg.chip.jpg\" ]; then\n",
    "\t\t    cnt=$((cnt+1))\n",
    "\t\t    echo \"Renaming 24_0_1_20170116220224657 .jpg.chip.jpg\\tto 24_0_1_20170116220224657.jpg.chip.jpg\"\n",
    "  \t\t\tmv \"$UTKDIR/24_0_1_20170116220224657 .jpg.chip.jpg\" \"$UTKDIR/24_0_1_20170116220224657.jpg.chip.jpg\"\n",
    "  \tfi\n",
    "\t\tif [ -f \"$UTKDIR/39_1_20170116174525125.jpg.chip.jpg\" ]; then\n",
    "\t\t    cnt=$((cnt+1))\n",
    "\t\t    echo \"Renaming 39_1_20170116174525125.jpg.chip.jpg\\tto 39_1_1_20170116174525125.jpg.chip.jpg\"\n",
    "  \t\t\tmv \"$UTKDIR/39_1_20170116174525125.jpg.chip.jpg\" \"$UTKDIR/39_1_1_20170116174525125.jpg.chip.jpg\"\n",
    "  \tfi\n",
    "\t\tif [ -f \"$UTKDIR/61_1_20170109150557335.jpg.chip.jpg\" ]; then\n",
    "\t\t    cnt=$((cnt+1))\n",
    "\t\t    echo \"Renaming 61_1_20170109150557335.jpg.chip.jpg\\tto 61_1_3_20170109150557335.jpg.chip.jpg\"\n",
    "  \t\t\tmv \"$UTKDIR/61_1_20170109150557335.jpg.chip.jpg\" \"$UTKDIR/61_1_3_20170109150557335.jpg.chip.jpg\"\n",
    "  \tfi\n",
    "\t\tif [ -f \"$UTKDIR/55_0_0_20170116232725357jpg.chip.jpg\" ]; then\n",
    "\t\t    cnt=$((cnt+1))\n",
    "\t\t    echo \"Renaming 55_0_0_20170116232725357jpg.chip.jpg\\tto 55_0_0_20170116232725357.jpg.chip.jpg\"\n",
    "  \t\t\tmv \"$UTKDIR/55_0_0_20170116232725357jpg.chip.jpg\" \"$UTKDIR/55_0_0_20170116232725357.jpg.chip.jpg\"\n",
    "  \tfi\n",
    "\t\tif [ -f \"$UTKDIR/61_1_20170109142408075.jpg.chip.jpg\" ]; then\n",
    "\t\t    cnt=$((cnt+1))\n",
    "\t\t    echo \"Renaming 61_1_20170109142408075.jpg.chip.jpg\\tto 61_1_1_20170109142408075.jpg.chip.jpg\"\n",
    "  \t\t\tmv \"$UTKDIR/61_1_20170109142408075.jpg.chip.jpg\" \"$UTKDIR/61_1_1_20170109142408075.jpg.chip.jpg\"\n",
    "  \tfi\n",
    "  \tif [ \"$cnt\" -eq 0 ]; then\n",
    "      echo \"Nothing to do; all files correctly named.\"\n",
    "  \tfi\n",
    "  else\n",
    "    \t  echo \"$UTKDIR directory not found.\"\n",
    "\tfi\n",
    "```\n",
    "\n",
    "Alternatively, you can apply the following corrections by hand:\n",
    "\n",
    "* `24_0_1_20170116220224657 .jpg.chip.jpg` -> `24_0_1_20170116220224657.jpg.chip.jpg` _(Space in filename)_\n",
    "* `55_0_0_20170116232725357jpg.chip.jpg` -> `55_0_0_20170116232725357.jpg.chip.jpg` _(Missing period)_\n",
    "* `61_1_20170109142408075.jpg.chip.jpg` -> `61_1_1_20170109142408075.jpg.chip.jpg` _(Missing gender)_\n",
    "* `61_1_20170109150557335.jpg.chip.jpg` -> `61_1_3_20170109150557335.jpg.chip.jpg` _(Missing race)_\n",
    "* `39_1_20170116174525125.jpg.chip.jpg` -> `39_1_1_20170116174525125.jpg.chip.jpg` _(Missing gender)_\n",
    "\n",
    "Once that is done, feel free to re-run the dataset validation script a few fields above to check that there ar eno more mismatches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Analysing and Filtering the Data\n",
    "\n",
    "Now let's look at the data we have; the following table shows the breakdown of the dataset by gender, age and race (we will not use the datetime metadata field).\n",
    "\n",
    "#### Original UTKFace Dataset\n",
    "\n",
    "| **Gender/Age** | **Asian** | **Black** | **Indian** | **Other** |  **White** |  **Total** |\n",
    "|:---------------|----------:|----------:|-----------:|----------:|-----------:|-----------:|\n",
    "| **Female**     | **1,859** | **2,210** |  **1,715** |   **932** |  **4,601** | **11,317** |\n",
    "| 1 - 14         |       461 |       110 |        297 |       280 |        783 |      1,931 |\n",
    "| 15 - 24        |       414 |       346 |        383 |       277 |        667 |      2,087 |\n",
    "| 25 - 44        |       872 |     1,473 |        847 |       346 |      1,655 |      5,193 |\n",
    "| 45 - 64        |        39 |       205 |        146 |        22 |        798 |      1,210 |\n",
    "| 65 and over    |        73 |        76 |         42 |         7 |        698 |        896 |\n",
    "| **Male**       | **1,575** | **2,318** |  **2,261** |   **760** |  **5,477** | **12,391** |\n",
    "| 1 - 14         |       511 |        87 |        246 |       163 |        713 |      1,720 |\n",
    "| 15 - 24        |       137 |       246 |        175 |       121 |        486 |      1,165 |\n",
    "| 25 - 44        |       601 |     1,426 |      1,102 |       374 |      2,056 |      5,559 |\n",
    "| 45 - 64        |       179 |       400 |        649 |        97 |      1,560 |      2,885 |\n",
    "| 65 and over    |       147 |       159 |         89 |         5 |        662 |      1,062 |\n",
    "| **Total**      | **3,434** | **4,528** |  **3,976** | **1,692** | **10,078** | **23,708** |\n",
    "\n",
    "After review of some sample images and discussion with various teammembers, we decided to discard images for children (age < 15) and older adults (age ≥ 65) as well as those where the race was classified as \"Other\" for consistency. This reduced the dataset to the following:\n",
    "\n",
    "#### Filtered UTKFace Dataset\n",
    "\n",
    "| **Gender/Age** | **Asian** | **Black** | **Indian** | **White** |  **Total** |\n",
    "|:---------------|----------:|----------:|-----------:|----------:|-----------:|\n",
    "| **Female**     | **1,325** | **2,024** |  **1,376** | **3,120** |  **7,845** |\n",
    "| 15 - 24        |       414 |       346 |        383 |       667 |      2,087 |\n",
    "| 25 - 44        |       872 |     1,473 |        847 |     1,655 |      5,193 |\n",
    "| 45 - 64        |        39 |       205 |        146 |       798 |      1,210 |\n",
    "| **Male**       |   **917** | **2,072** |  **1,926** | **4,102** |  **9,017** |\n",
    "| 15 - 24        |       137 |       246 |        175 |       486 |      1,165 |\n",
    "| 25 - 44        |       601 |     1,426 |      1,102 |     2,056 |      5,559 |\n",
    "| 45 - 64        |       179 |       400 |        649 |     1,560 |      2,885 |\n",
    "| **Total**      | **2,242** | **4,096** |  **3,302** | **7,222** | **16,862** |\n",
    "\n",
    "We can use the `preselect_landmarks` function (below) to filter out images where the subject's age is under 15 or above 64 and their race is classified as 'other'. The script will produce a file called `ll_initial.txt` in the current directory listing the 16,862 images that fit that criteria. (Note that we specify the seed value through the `randomseed` parameter to enable reproducibility.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'landmark_list_corrected.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 161>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(remainder_landmarks),\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mremainder images saved to file\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m                   remainder_landmarks_file)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# Selecting images for people of known races and with ages between 15-64.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=160'>161</a>\u001b[0m preselect_landmarks(\u001b[39m'\u001b[39;49m\u001b[39mlandmark_list_corrected.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, age\u001b[39m=\u001b[39;49m(\u001b[39m15\u001b[39;49m, \u001b[39m64\u001b[39;49m),\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m                     race\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39masian\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mblack\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mindian\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwhite\u001b[39;49m\u001b[39m'\u001b[39;49m], filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mll_initial\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m                     randomise\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, randomseed\u001b[39m=\u001b[39;49m\u001b[39m680780122122\u001b[39;49m)\n",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 10\u001b[0m in \u001b[0;36mpreselect_landmarks\u001b[0;34m(landmarks_file, age, gender, race, log, randomise, randomseed, target, filename)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreselect_landmarks\u001b[39m(landmarks_file: \u001b[39mstr\u001b[39m, age\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gender\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, race\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                         \u001b[39m*\u001b[39m, log: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, randomise: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                         randomseed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, target\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\"\" Preselect Landmarks\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    Iterates through an original file listing images and associated landmarks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m    :return: nothing, may print Errors, Warnings and log messages to stdout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(landmarks_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         landmarks \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Initialise maps\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'landmark_list_corrected.txt'"
     ]
    }
   ],
   "source": [
    "from sys import exit\n",
    "from os.path import exists\n",
    "\n",
    "def preselect_landmarks(landmarks_file: str, age=None, gender=None, race=None,\n",
    "                        *, log: bool = False, randomise: bool = False,\n",
    "                        randomseed=None, target=None, filename=None) -> None:\n",
    "    \"\"\" Preselect Landmarks\n",
    "\n",
    "    Iterates through an original file listing images and associated landmarks\n",
    "    applying the given filters and generates another file with the subset of\n",
    "    images that passed *all* filters.\n",
    "\n",
    "    :param landmarks_file: name of original landmark file (str, Required)\n",
    "    :param log: Whether to log why each image was discarded and to print a\n",
    "        summary message with the number of images filtered (Default: False)\n",
    "    :param target: (maximum) number of images to select based on the provided\n",
    "        filters. If defined, the function may output two files with suffixes:\n",
    "        • \"_filtered\": list of images which meet all filter criteria;\n",
    "        • \"_remainder\": list of images which do not meet filter criteria or\n",
    "            exceed target number;\n",
    "    :param randomise: shuffle landmarks before preselection? (Default: False)\n",
    "    :param randomseed: seed value (int) when randomising (Default: None)\n",
    "    :param age: tuple containing min (int) and max (int) values (Default: None)\n",
    "    :param gender: 'male' or 'female' (str, Default: None)\n",
    "    :param race: either a str with a single race or a list for multiple races,\n",
    "        values 'white', 'black', 'asian', 'indian' and 'other' (Default: None)\n",
    "    :param filename: filename for target files\n",
    "    :return: nothing, may print Errors, Warnings and log messages to stdout\n",
    "    \"\"\"\n",
    "    with open(landmarks_file, 'r') as file:\n",
    "        landmarks = file.readlines()\n",
    "\n",
    "    # Initialise maps\n",
    "    genders: dict[str: str] = {\n",
    "        '0': \"male\",\n",
    "        '1': \"female\"\n",
    "    }\n",
    "    races: dict[str: str] = {\n",
    "        '0': \"white\",\n",
    "        '1': \"black\",\n",
    "        '2': \"asian\",\n",
    "        '3': \"indian\",\n",
    "        '4': \"other\"\n",
    "    }\n",
    "\n",
    "    # Read parameters for valid filters and capture those for new filename\n",
    "    filters = []\n",
    "    if isinstance(age, tuple) and age[0] <= age[1]:\n",
    "        filters.append(str(age[0]) + \"-\" + str(age[1]))\n",
    "    if isinstance(gender, str) and gender in genders.values():\n",
    "        filters.append(gender)\n",
    "    if isinstance(race, str):\n",
    "        race = [race]\n",
    "    if isinstance(race, list) and all(r in races.values() for r in race):\n",
    "        filters.append(\"-\".join(race))\n",
    "\n",
    "    # Abort if no valid filters were found or invalid target\n",
    "    if len(filters) == 0:\n",
    "        print(\"Error: No valid filters to apply.\")\n",
    "        exit(1)\n",
    "    if target is not None and (not isinstance(target, int) or target < 1):\n",
    "        print(\"Error: Target needs to be greater than zero.\")\n",
    "        exit(1)\n",
    "\n",
    "    # Abort if files already exist with target name (avoid overwriting).\n",
    "    if filename:\n",
    "        filtered_landmarks_file = filename\n",
    "    else:\n",
    "        filtered_landmarks_file = landmarks_file.split(\".\")[0]\n",
    "        filtered_landmarks_file += \"_\" + \"_\".join(filters)\n",
    "    filtered_landmarks_file += \"_filtered\" if target else \"\"\n",
    "    filtered_landmarks_file += \".txt\"\n",
    "    if exists(filtered_landmarks_file):\n",
    "        print(f\"Error: File '{filtered_landmarks_file}' already exists in \"\n",
    "              f\"current directory; delete or rename and run script again.\")\n",
    "        exit(1)\n",
    "    if filename:\n",
    "        remainder_landmarks_file = filename\n",
    "    else:\n",
    "        remainder_landmarks_file = landmarks_file.split(\".\")[0]\n",
    "        remainder_landmarks_file += \"_\" + \"_\".join(filters)\n",
    "    remainder_landmarks_file += \"_remainder.txt\"\n",
    "    if target and exists(remainder_landmarks_file):\n",
    "        print(\n",
    "            f\"Error: File '{remainder_landmarks_file}' already exists in \"\n",
    "            f\"current directory; delete or rename and run script again.\")\n",
    "        exit(1)\n",
    "\n",
    "    if randomise:\n",
    "        if seed is not None:\n",
    "            seed(randomseed)\n",
    "        shuffle(landmarks)\n",
    "\n",
    "    filtered_landmarks: list[str] = []\n",
    "    remainder_landmarks: list[str] = []\n",
    "    for line in landmarks:\n",
    "        # Iterate over all lines in landmark file and apply filters\n",
    "        keep = True\n",
    "\n",
    "        # Retrieve metadata from filename\n",
    "        imagename = line.split()[0]\n",
    "        splitname: list[str] = imagename.split(\".\")[0].split(\"_\")\n",
    "        # 0 is presumed if no age is provided in metadata, so this may fail\n",
    "        # minimum age filters greater than 0.\n",
    "        line_age = int(splitname[0]) if len(splitname) > 0 else 0\n",
    "        # An image with no gender metadata will fail all gender filters\n",
    "        if len(splitname) > 1 and splitname[1] in genders:\n",
    "            line_gender = genders[splitname[1]]\n",
    "        else:\n",
    "            line_gender = \"\"\n",
    "        # An image without race metadata will fail any race filters\n",
    "        if len(splitname) > 2 and splitname[2] in races:\n",
    "            line_race = races[splitname[2]]\n",
    "        else:\n",
    "            line_race = \"\"\n",
    "\n",
    "        # Check if a given line passes *all* filters\n",
    "        if isinstance(age, tuple) and (line_age < age[0] or line_age > age[1]):\n",
    "            if log:\n",
    "                print(f\"Image {imagename} skipped due to age ({line_age}).\", )\n",
    "            keep = False\n",
    "        if isinstance(gender, str) and line_gender != gender:\n",
    "            if log:\n",
    "                print(f\"Image {imagename} skipped due to gender ({line_gender}).\", )\n",
    "            keep = False\n",
    "        if isinstance(race, list) and line_race not in race:\n",
    "            if log:\n",
    "                print(f\"Image {imagename} skipped due to race ({line_race}).\", )\n",
    "            keep = False\n",
    "\n",
    "        if keep and (target is None or target > len(filtered_landmarks)):\n",
    "            filtered_landmarks.append(line)\n",
    "            if log:\n",
    "                print(f\"Image {imagename} added to filtered list.\")\n",
    "        else:\n",
    "            remainder_landmarks.append(line)\n",
    "            if log and target:\n",
    "                print(f\"Image {imagename} added to remainder list.\")\n",
    "\n",
    "    if len(filtered_landmarks) == 0:\n",
    "        print(\"Warning: No images passed all filters.\")\n",
    "        exit(1)\n",
    "\n",
    "    with open(filtered_landmarks_file, 'w') as file:\n",
    "        file.writelines(filtered_landmarks)\n",
    "    if log:\n",
    "        print(len(filtered_landmarks),\n",
    "              \"filtered images saved to file\",\n",
    "              filtered_landmarks_file)\n",
    "\n",
    "    if target and len(remainder_landmarks) != 0:\n",
    "        with open(remainder_landmarks_file, 'w') as file:\n",
    "            file.writelines(remainder_landmarks)\n",
    "        if log:\n",
    "            print(len(remainder_landmarks),\n",
    "                  \"remainder images saved to file\",\n",
    "                  remainder_landmarks_file)\n",
    "\n",
    "\n",
    "# Selecting images for people of known races and with ages between 15-64.\n",
    "preselect_landmarks('landmark_list_corrected.txt', age=(15, 64),\n",
    "                    race=['asian', 'black', 'indian', 'white'], filename='ll_initial',\n",
    "                    randomise=True, randomseed=680780122122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into Training, Validation and Test datasets\n",
    "\n",
    "The plan is to train our models on 3 datasets with differing ratios of female/male images and then test their performance against female-only, male-only and evenly-split datasets. This is the plan:\n",
    "\n",
    "![data_preparation_plan](jupyter_images/Data_Preparation_Plan.png)\n",
    "\n",
    "First, let's run `preselect_landmarks` two more times to produce a file with 7,500 randomly-ordered males and another with 7,500 randomly-ordered females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preselect_landmarks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1724/1928435526.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Selecting 7,500 male images from the initial dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreselect_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'll_initial.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"male\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m680780122122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Selecting 7,500 female images from the initial dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpreselect_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'll_initial_male_remainder.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"female\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m680780122122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preselect_landmarks' is not defined"
     ]
    }
   ],
   "source": [
    "# Selecting 7,500 male images from the initial dataset\n",
    "preselect_landmarks('ll_initial.txt', gender=\"male\", target=7500, randomise=True, randomseed=680780122122)\n",
    "\n",
    "# Selecting 7,500 female images from the initial dataset\n",
    "preselect_landmarks('ll_initial_male_remainder.txt', gender=\"female\", target=7500, randomise=True, randomseed=680780122122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easier to follow, let's import the `os` package and use it to rename the files to `ll_males.txt` and `ll_females.txt`, respectively. We can also delete the intermediary/additional files generated so far (and will do so at each step from now on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'll_initial_male_filtered.txt' -> 'll_males.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Renaming the output files for clarity\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m os\u001b[39m.\u001b[39;49mrename(\u001b[39m'\u001b[39;49m\u001b[39mll_initial_male_filtered.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mll_males.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39mrename(\u001b[39m'\u001b[39m\u001b[39mll_initial_male_remainder_female_filtered.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mll_females.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Deleting intermediary files\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'll_initial_male_filtered.txt' -> 'll_males.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_initial_male_filtered.txt', 'll_males.txt')\n",
    "os.rename('ll_initial_male_remainder_female_filtered.txt', 'll_females.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('landmark_list_concatenated.txt')\n",
    "os.remove('landmark_list_corrected.txt')\n",
    "os.remove('ll_initial.txt')\n",
    "os.remove('ll_initial_male_remainder.txt')\n",
    "os.remove('ll_initial_male_remainder_female_remainder.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract 750 lines (10%) from `ll_females.txt` and `ll_males.txt` and concatenate that into a single balanced test file called `ll_test_50-50_split.txt` with 1,500 images. We will keep all three files for our testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'll_males.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Splitting the male data set into test (750 images) and training/validation (6,750 images)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m preselect_landmarks(\u001b[39m'\u001b[39;49m\u001b[39mll_males.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mll_males_test\u001b[39;49m\u001b[39m\"\u001b[39;49m, gender\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmale\u001b[39;49m\u001b[39m\"\u001b[39;49m, target\u001b[39m=\u001b[39;49m\u001b[39m750\u001b[39;49m, randomise\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, randomseed\u001b[39m=\u001b[39;49m\u001b[39m680780122122\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Splitting the female data set into test (750 images) and training/validation (6,750 images)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m preselect_landmarks(\u001b[39m'\u001b[39m\u001b[39mll_females.txt\u001b[39m\u001b[39m'\u001b[39m, filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mll_females_test\u001b[39m\u001b[39m\"\u001b[39m, gender\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfemale\u001b[39m\u001b[39m\"\u001b[39m, target\u001b[39m=\u001b[39m\u001b[39m750\u001b[39m, randomise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, randomseed\u001b[39m=\u001b[39m\u001b[39m680780122122\u001b[39m)\n",
      "\u001b[1;32m/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb Cell 16\u001b[0m in \u001b[0;36mpreselect_landmarks\u001b[0;34m(landmarks_file, age, gender, race, log, randomise, randomseed, target, filename)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreselect_landmarks\u001b[39m(landmarks_file: \u001b[39mstr\u001b[39m, age\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, gender\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, race\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                         \u001b[39m*\u001b[39m, log: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, randomise: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                         randomseed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, target\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\"\" Preselect Landmarks\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    Iterates through an original file listing images and associated landmarks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m    :return: nothing, may print Errors, Warnings and log messages to stdout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(landmarks_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m         landmarks \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/jamie/OneDrive/Desktop/COMP9444/Project/dr-teeth-and-the-electric-mayhem/Data_Preparation.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Initialise maps\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'll_males.txt'"
     ]
    }
   ],
   "source": [
    "# Splitting the male data set into test (750 images) and training/validation (6,750 images)\n",
    "preselect_landmarks('ll_males.txt', filename=\"ll_males_test\", gender=\"male\", target=750, randomise=True, randomseed=680780122122)\n",
    "\n",
    "# Splitting the female data set into test (750 images) and training/validation (6,750 images)\n",
    "preselect_landmarks('ll_females.txt', filename=\"ll_females_test\", gender=\"female\", target=750, randomise=True, randomseed=680780122122)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_males_test_filtered.txt', 'll_test_males.txt')\n",
    "os.rename('ll_females_test_filtered.txt', 'll_test_females.txt')\n",
    "\n",
    "# Concatenating the male/female test images into a single dataset\n",
    "concatenate_files(\"ll_test_50-50_split.txt\", files=['ll_test_males.txt', 'll_test_females.txt'], delete=False)\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_males.txt')\n",
    "os.remove('ll_females.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the remaining male/female data into Training (80%: 6,000 images) and Validation (10%: 750 images) sets, but we won't contatenate them just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the male data set into training (6,000 images) and validation (750 images)\n",
    "preselect_landmarks('ll_males_test_remainder.txt', filename=\"ll_males_validation\", gender=\"male\", target=750)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_males_validation_filtered.txt', 'll_males_validation.txt')\n",
    "os.rename('ll_males_validation_remainder.txt', 'll_males_training.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_males_test_remainder.txt')\n",
    "\n",
    "# Splitting the female data set into training (6,000 images) and validation (750 images)\n",
    "preselect_landmarks('ll_females_test_remainder.txt', filename=\"ll_females_validation\", gender=\"female\", target=750)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_females_validation_filtered.txt', 'll_females_validation.txt')\n",
    "os.rename('ll_females_validation_remainder.txt', 'll_females_training.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_females_test_remainder.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to split both the male/female **training** sets into thirds so that we can compose our 3 separate cohorts (25-75 split, 50-50 split, 75-25 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files successfully concatenated as ll_training_25-75_split.txt\n",
      "Input files successfully concatenated as ll_training_50-50_split.txt\n",
      "Input files successfully concatenated as ll_training_75-25_split.txt\n"
     ]
    }
   ],
   "source": [
    "# Spliting the male training dataset into 3 files with 2,000 images each\n",
    "preselect_landmarks('ll_males_training.txt', filename=\"ll_males_training_1\", gender=\"male\", target=2000)\n",
    "preselect_landmarks('ll_males_training_1_remainder.txt', filename=\"ll_males_training_2\", gender=\"male\", target=2000)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_males_training_1_filtered.txt', 'll_males_training_cohort_1.txt')\n",
    "os.rename('ll_males_training_2_filtered.txt', 'll_males_training_cohort_2.txt')\n",
    "os.rename('ll_males_training_2_remainder.txt', 'll_males_training_cohort_3.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_males_training.txt')\n",
    "os.remove('ll_males_training_1_remainder.txt')\n",
    "\n",
    "# Spliting the female training dataset into 3 files with 2,000 images each\n",
    "preselect_landmarks('ll_females_training.txt', filename=\"ll_females_training_1\", gender=\"female\", target=2000)\n",
    "preselect_landmarks('ll_females_training_1_remainder.txt', filename=\"ll_females_training_2\", gender=\"female\", target=2000)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_females_training_1_filtered.txt', 'll_females_training_cohort_1.txt')\n",
    "os.rename('ll_females_training_2_filtered.txt', 'll_females_training_cohort_2.txt')\n",
    "os.rename('ll_females_training_2_remainder.txt', 'll_females_training_cohort_3.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_females_training.txt')\n",
    "os.remove('ll_females_training_1_remainder.txt')\n",
    "\n",
    "# 25:75 Training file: 1 female cohort + 3 male cohorts\n",
    "concatenate_files('ll_training_25-75_split.txt', files=['ll_females_training_cohort_3.txt', 'll_males_training_cohort_1.txt', 'll_males_training_cohort_2.txt', 'll_males_training_cohort_3.txt'])\n",
    "\n",
    "# 50:50 Training file: 2 female cohorts + 2 male cohorts\n",
    "concatenate_files('ll_training_50-50_split.txt', files=['ll_females_training_cohort_1.txt', 'll_females_training_cohort_2.txt', 'll_males_training_cohort_1.txt', 'll_males_training_cohort_2.txt'])\n",
    "\n",
    "# 75:25 Training file: 3 female cohorts + 1 male cohort\n",
    "concatenate_files('ll_training_75-25_split.txt', files=['ll_females_training_cohort_1.txt', 'll_females_training_cohort_2.txt', 'll_females_training_cohort_3.txt','ll_males_training_cohort_3.txt'])\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_females_training_cohort_1.txt')\n",
    "os.remove('ll_females_training_cohort_2.txt')\n",
    "os.remove('ll_females_training_cohort_3.txt')\n",
    "os.remove('ll_males_training_cohort_1.txt')\n",
    "os.remove('ll_males_training_cohort_2.txt')\n",
    "os.remove('ll_males_training_cohort_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same with the male/female **validation** sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files successfully concatenated as ll_validation_25-75_split.txt\n",
      "Input files successfully concatenated as ll_validation_50-50_split.txt\n",
      "Input files successfully concatenated as ll_validation_75-25_split.txt\n"
     ]
    }
   ],
   "source": [
    "# Spliting the male validation dataset into 3 files with 250 images each\n",
    "preselect_landmarks('ll_males_validation.txt', filename=\"ll_males_validation_1\", gender=\"male\", target=250)\n",
    "preselect_landmarks('ll_males_validation_1_remainder.txt', filename=\"ll_males_validation_2\", gender=\"male\", target=250)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_males_validation_1_filtered.txt', 'll_males_validation_cohort_1.txt')\n",
    "os.rename('ll_males_validation_2_filtered.txt', 'll_males_validation_cohort_2.txt')\n",
    "os.rename('ll_males_validation_2_remainder.txt', 'll_males_validation_cohort_3.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_males_validation.txt')\n",
    "os.remove('ll_males_validation_1_remainder.txt')\n",
    "\n",
    "# Spliting the female validation dataset into 3 files with 250 images each\n",
    "preselect_landmarks('ll_females_validation.txt', filename=\"ll_females_validation_1\", gender=\"female\", target=250)\n",
    "preselect_landmarks('ll_females_validation_1_remainder.txt', filename=\"ll_females_validation_2\", gender=\"female\", target=250)\n",
    "\n",
    "# Renaming the output files for clarity\n",
    "os.rename('ll_females_validation_1_filtered.txt', 'll_females_validation_cohort_1.txt')\n",
    "os.rename('ll_females_validation_2_filtered.txt', 'll_females_validation_cohort_2.txt')\n",
    "os.rename('ll_females_validation_2_remainder.txt', 'll_females_validation_cohort_3.txt')\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_females_validation.txt')\n",
    "os.remove('ll_females_validation_1_remainder.txt')\n",
    "\n",
    "# 25:75 Validation file: 1 female cohort + 3 male cohorts\n",
    "concatenate_files('ll_validation_25-75_split.txt', files=['ll_females_validation_cohort_3.txt', 'll_males_validation_cohort_1.txt', 'll_males_validation_cohort_2.txt', 'll_males_validation_cohort_3.txt'])\n",
    "\n",
    "# 50:50 Validation file: 2 female cohorts + 2 male cohorts\n",
    "concatenate_files('ll_validation_50-50_split.txt', files=['ll_females_validation_cohort_1.txt', 'll_females_validation_cohort_2.txt', 'll_males_validation_cohort_1.txt', 'll_males_validation_cohort_2.txt'])\n",
    "\n",
    "# 75:25 Validation file: 3 female cohorts + 1 male cohort\n",
    "concatenate_files('ll_validation_75-25_split.txt', files=['ll_females_validation_cohort_1.txt', 'll_females_validation_cohort_2.txt', 'll_females_validation_cohort_3.txt','ll_males_validation_cohort_3.txt'])\n",
    "\n",
    "# Deleting intermediary files\n",
    "os.remove('ll_females_validation_cohort_1.txt')\n",
    "os.remove('ll_females_validation_cohort_2.txt')\n",
    "os.remove('ll_females_validation_cohort_3.txt')\n",
    "os.remove('ll_males_validation_cohort_1.txt')\n",
    "os.remove('ll_males_validation_cohort_2.txt')\n",
    "os.remove('ll_males_validation_cohort_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now have the following files ready for training, validation and testing:\n",
    "\n",
    "#### Training\n",
    "* 25% Female + 75% Male: `ll_training_25-75_split.txt` (8,000 lines)\n",
    "* 50% Female + 50% Male: `ll_training_50-50_split.txt` (8,000 lines)\n",
    "* 75% Female + 25% Male: `ll_training_75-25_split.txt` (8,000 lines)\n",
    "\n",
    "#### Validation\n",
    "* 25% Female + 75% Male: `ll_validation_25-75_split.txt` (1,000 lines)\n",
    "* 50% Female + 50% Male: `ll_validation_50-50_split.txt` (1,000 lines)\n",
    "* 75% Female + 25% Male: `ll_validation_75-25_split.txt` (1,000 lines)\n",
    "\n",
    "#### Testing\n",
    "* 100% Female: `ll_test_females.txt` (750 lines)\n",
    "* 100% Male: `ll_test_males.txt` (750 lines)\n",
    "* 50% Female + 50% Male: `ll_test_50-50_split.txt` (1,500 lines)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "To examine the effects of different model architectures on model bias we created three different models to compare. These models were:\n",
    "\n",
    "* A basic convolutional neural network\n",
    "* A dense net\n",
    "* A residual network\n",
    "\n",
    "Each of these models would then be trained on three different training datasets. These datasets will have varying proportions of men and women and will be validated using a datset with similar proportions. This allows us to examine how resilient each model is to dataset bias.\n",
    "\n",
    "##### Basic Convolutional NN\n",
    "\n",
    "The first NN used in our experiment is a basic NN featuring four convolutional layers, one linear layer and pooling and dropout layers.\n",
    "\n",
    "![alternative text](jupyter_images/convNN2_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNN2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convNN2, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 6)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc1(x) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "\n",
    "A next step after the ConvNN2 model was the Resnet34 model. We decided on the Resnet model because of its good performance for general image classification, and we wished to see how it might be able to extend to facial landmark detection. The skip connections in the Resnet models also allowed us to train a larger network without running into vanishing gradient problems that we might have encountered had we simply naively added extra layers to our convNN2 model. In addition to Resnet34, we did test other variants including Resnet18 and Resnet50, but found that having 34 layers struck a good balance between performance and time required for training. \n",
    "\n",
    "Given that we decided to select three landmarks for our points of reference on the face, any additional resnet layers did not yield significant reductions in error rate. In the future if we wished to do our testing with additional facial landmarks, then it may be necessary to use Resnet101 or Resnet152."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![foo](jupyter_images/resnet-table.png)\n",
    "\n",
    "Source: He, Kaiming; Zhang, Xiangyu; Ren, Shaoqing; Sun, Jian (2016). *Deep Residual Learning for Image Recognition*\n",
    "\n",
    "Structure of ResNet34\n",
    "\n",
    "![foo](jupyter_images/resnetdiagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet18, self).__init__()\n",
    "        self.resnet = torchvision.models.ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2], num_classes=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet.forward(x)\n",
    "\n",
    "class resnet34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet34, self).__init__()\n",
    "        self.resnet = torchvision.models.ResNet(torchvision.models.resnet.BasicBlock, [3, 4, 6, 3], num_classes=6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet.forward(x)\n",
    "\n",
    "class resnet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet50, self).__init__()\n",
    "        self.resnet = torchvision.models.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], num_classes=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pytorch implementation of the Resnet models allowed for defining a specific number of output classes. Therefore the Resnet models used in our experiments uses 6 output classes, one for each coordinate of the three chosen landmark features. Thus the fully connected layer at the end of the Resnet is no longer a 1000 output fully connected layer, which the original authors of the Resnet paper used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Net\n",
    "\n",
    "This network is based on the pytorch implementation of [densenet121](https://pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html). A dense net was chosen as it is a common network used in image recognition and classification tasks.\n",
    "\n",
    "Since the default pytorch implementation produces an output tensor of shape [1000,] a few linear layers were added to turn this into a [6,] output in order to describe just three landmarks.\n",
    "\n",
    "The structure is as follows below\n",
    "\n",
    "![foo](jupyter_images/densenetdiagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseNN(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(denseNN, self).__init__()\n",
    "        # Pytorch does not come with densenet121 installed and it must be downloaded.\n",
    "        # \n",
    "        self.dense121 = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n",
    "        self.fc1 = nn.Linear(1000, 600)\n",
    "        self.fc2 = nn.Linear(600, 100)\n",
    "        self.fc3 = nn.Linear(100, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense121(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process and Validation\n",
    "\n",
    "To perform our training we must first create a pytorch [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). This allows us to read in images and labels described by the dataset text files created earlier. This class also serves to extract other information from the name of the image. Features like age, race and gender are recorded in the image name. Some preprocessing is also required. Once images are loaded into a tensor, we divide all values by 255 to restrict each value to the range [0, 1]. If this is not done we run into issues training where the loss and some weights could reach infinity and cause issues in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from torch import tensor, div\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Image Dataset Class\n",
    "    \"\"\"\n",
    "    def __init__(self, annotations_file: str, img_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        Create a Custom Image Dataset object\n",
    "\n",
    "        Usage:\n",
    "        dataset_obj = CustomImageDataset('landmarks.txt', '../images/')\n",
    "\n",
    "        :param annotations_file: address of input file (relative to script)\n",
    "        :param img_dir: path to image directory (relative to script)\n",
    "        :param transform: function to be applied to every image requested\n",
    "        \"\"\"\n",
    "\n",
    "        # Read the landmarks file for later querying\n",
    "        with open(annotations_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        self.img_labels = [line.split() for line in lines]\n",
    "        self.img_dir: str = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Initialise Maps for the __getitem__ method\n",
    "        self.genders: dict[str: str] = {\n",
    "            '0': \"male\",\n",
    "            '1': \"female\"\n",
    "        }\n",
    "        self.races: dict[str: str] = {\n",
    "            '0': \"white\",\n",
    "            '1': \"black\",\n",
    "            '2': \"asian\",\n",
    "            '3': \"indian\",\n",
    "            '4': \"other\"\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of images in the Custom Image Dataset object.\n",
    "\n",
    "        Usage:\n",
    "        len(dataset_obj)\n",
    "\n",
    "        :return: int\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Used by PyTorch to request a given image within the Dataset.\n",
    "\n",
    "        Usage:\n",
    "        dataset_obj[42]\n",
    "\n",
    "        :param idx: number of requested image (should be less than __len__)\n",
    "        :return: the requested image and its metadata as separate variables:\n",
    "            'image': Scaled PyTorch tensor obj for the image file\n",
    "            'age': int with the age of the person in the image\n",
    "            'gender': str ('male' or 'female')\n",
    "            'race': str ('white', 'black', 'asian', 'indian', 'others')\n",
    "            'landamarks': PyTorch tensor obj with 68 pairs of x,y coords\n",
    "        \"\"\"\n",
    "\n",
    "        # Reads file for given index as a tensor image\n",
    "        imagename = self.img_labels[idx][0] + \".chip.jpg\"\n",
    "        image = read_image(path.join(self.img_dir, imagename)).float()\n",
    "        image_scaled = div(image, 255)\n",
    "\n",
    "        # Applies any transformations to image\n",
    "        if self.transform:\n",
    "            image_scaled = self.transform(image_scaled)\n",
    "\n",
    "        # Reads the image metadata from the filename\n",
    "        splitname: list[str] = imagename.split(\".\")[0].split(\"_\")\n",
    "        age: int = int(splitname[0])\n",
    "        gender: str = self.genders[splitname[1]]\n",
    "        race: str = self.races[splitname[2]]\n",
    "        # datetime: int = int(splitname[3][:13])    // Not used\n",
    "\n",
    "        # Reorganises the x,y landmark coordinates as a 68x2 tensor\n",
    "        coords = self.img_labels[idx][1:]\n",
    "        raw_landmarks: list[list[int, int]] = []\n",
    "        for i in range(0, len(coords), 2):\n",
    "            raw_landmarks.append([int(coords[i]), int(coords[i + 1])])\n",
    "        landmarks = tensor(raw_landmarks).float()\n",
    "\n",
    "        return image_scaled, age, gender, race, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        current_time = time.time()\n",
    "\n",
    "        duration = current_time - self.start_time\n",
    "    \n",
    "        hours = int(duration / 3600)\n",
    "        minutes = int((duration % 3600) / 60)\n",
    "        seconds = int((duration % 3600) % 60)\n",
    "\n",
    "        return f\"{hours}h {minutes}m {seconds}s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation\n",
    "\n",
    "The evaluate function is used for both validation and testing purposes. Here we calculate the straight line distance between the real label coordinates and the coordinates outputted by the model.\n",
    "\n",
    "Average distance was chosen over MSE or other similar error functions as it is easier to interpret.\n",
    "\n",
    "The evaluate function also outputs the standard deviation of these scores, as it may provide some useful information about the variance of the model. A high variance would mean the model is simply guessing in a tight area and not producing unique coordinates for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_set_path, device):\n",
    "    UTKFace = CustomImageDataset(valid_set_path, 'UTKFace')\n",
    "    valid_set = DataLoader(UTKFace, \n",
    "                            500, \n",
    "                            shuffle=True)\n",
    "\n",
    "    # We're calculating the distance ourselves as using MSE loss doesnt \n",
    "    # allow us to square root terms individually.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _, landmarks in valid_set:\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            outputs = model(images).view([-1,3,2]) # organise into (x, y) pairs\n",
    "\n",
    "            land_idx = [8, 30, 39]  # The labels we are training for\n",
    "            difference = torch.square(outputs - landmarks[:, land_idx]).to(device)\n",
    "            difference = torch.sqrt(difference[:, 0] + difference[:, 1])\n",
    "\n",
    "    model.train()\n",
    "    return torch.mean(difference).item(), torch.std(difference).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now for the training\n",
    "\n",
    "Here validation is only performed every 20 iterations as it adds a significant amount of time to the training process. Validating less frequently allows us to graph the results. Each time validation is performed, we check the current set of weights against the previous best scoring model. If the current weights perform better, we update the best model accordingly and record which iteration it occured in. \n",
    "\n",
    "Stochastic gradient descent (SGD) does not ensure that the final epoch and iteration will produce the best model. As such, validating regularly allows us to choose the best model thus far. While we cannot do this every iteration due to the overhead, we assume that validating regularly will yield a set of weights close to the optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, lr, device, valid_set, epochs=5):\n",
    "    loss_func = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # Initialise somewhere to save data for later graphing\n",
    "    batches = len(train_loader)\n",
    "    scores = np.empty([batches * epochs, 3])\n",
    "    scores[:] = np.nan\n",
    "\n",
    "    best_model = model\n",
    "    best_scores = {\"iteration\": 0, \n",
    "                \"mean\": 1000,\n",
    "                \"std\": 1000,\n",
    "                \"loss_list\": []}\n",
    "\n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            images, _, _, _, landmarks = data   # images, age, gender, race, landmarks\n",
    "\n",
    "            # Zero paramter gradients\n",
    "            optimizer.zero_grad()\n",
    "            images, landmarks = images.to(device), landmarks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            land_idx = [8, 30, 39]  # The indexs of the landmarks we are training with\n",
    "            loss = loss_func(outputs, landmarks[:, land_idx].view(-1, 6))\n",
    "            best_scores[\"loss_list\"].append(loss.item())    # Record for graphing later\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation is performed every 20 iterations due to its high overhead.\n",
    "            if i % 20 == 0:\n",
    "                mean, std = evaluate(model, valid_set, device)\n",
    "                scores[(epoch * batches) + i, 0] = (epoch * batches) + i\n",
    "                scores[(epoch * batches) + i, 1] = mean\n",
    "                scores[(epoch * batches) + i, 2] = std\n",
    "                print(f\"[{timer.elapsed_time()}] Epoch: {epoch}, iteration: {i}, loss: {loss.item()}, mean: {mean}, std: {std}\")\n",
    "\n",
    "                # If the current model is the best we have seen so far, preserver the weights\n",
    "                if mean < best_scores[\"mean\"]:\n",
    "                    best_model = copy.deepcopy(model)    # We need to copy to preserve weights\n",
    "                    best_scores[\"iteration\"] = (epoch * batches) + i\n",
    "                    best_scores[\"mean\"] = mean\n",
    "                    best_scores[\"std\"] = std\n",
    "            \n",
    "    # Remove iterations where we did not do any validation\n",
    "    filtered_scores = scores[~np.isnan(scores).any(axis=1)]\n",
    "\n",
    "    return best_model, best_scores, filtered_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used cli arguments to make the code in this notebook we can instead add the arguments to a string and pass the string to the `ArgumentParser` class.\n",
    "\n",
    "* -f, --train_file\n",
    "    - Used to specify the path to a training file. These should be text files with a list of images. For example `ll_training_75-25_split.txt` indicates the model should use the training set with the 75-25 gender split.\n",
    "* -vf, --validation_file\n",
    "    - Like the train file, this argument is used to indicate which subset of `UTKFace` to use for validation.\n",
    "* -b, --batch\n",
    "    - Set the batch size for training. All models were trained on a batch size of 32.\n",
    "* -m, --model\n",
    "    - Choose which model to train.\n",
    "    - The models availible are:\n",
    "        - \"convNN2\"\n",
    "        - \"dense\"\n",
    "        - \"resnet18\"\n",
    "        - \"resnet34\"\n",
    "        - \"resnet50\"\n",
    "* -lr, --learning_rate\n",
    "    - Specify the learning rate.\n",
    "* --cuda\n",
    "    - Including this argument will enable training on cuda.\n",
    "* -e, --epochs\n",
    "    - Specify the number of epochs to train for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_str = \"train.py -b 32 --learning_rate 0.0001 --cuda --train_file ll_training_75-25_split.txt --validation_file ll_validation_75-25_split.txt --epochs 50 -m dense\"\n",
    "\n",
    "# Read in args\n",
    "parser = ArgumentParser(arg_str)\n",
    "parser.add_argument(\"-tf\", \"--train_file\",\n",
    "                    help=\"Path to data file.\", \n",
    "                    metavar=\"FILE_PATH\", \n",
    "                    default=\"landmark_list.txt\")\n",
    "parser.add_argument(\"-vf\", \"--validation_file\",\n",
    "                    help=\"Choose file to use for validation.\",\n",
    "                    metavar=\"FILE_PATH\",\n",
    "                    default=\"landmark_list.txt\")\n",
    "parser.add_argument(\"-b\", \"--batch\", \n",
    "                    help=\"Batch size for training.\", \n",
    "                    type=int, \n",
    "                    metavar=\"INT\",\n",
    "                    default=64)\n",
    "parser.add_argument(\"-m\", \"--model\",\n",
    "                    help=\"Choose which model structure to use.\",\n",
    "                    default=\"convNN2\",\n",
    "                    metavar=\"MODEL_NAME\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\",\n",
    "                    help=\"Learning rate to run the optimizer function with.\",\n",
    "                    default=0.0001,\n",
    "                    type=float,\n",
    "                    metavar=\"FLOAT\")\n",
    "parser.add_argument(\"--cuda\",\n",
    "                    help=\"Add this argument to run the code using GPU acceleration.\",\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument(\"-e\", \"--epochs\",\n",
    "                    help=\"Dictate number of epochs to train for.\",\n",
    "                    type=int,\n",
    "                    metavar=\"INT\",\n",
    "                    default=5)\n",
    "\n",
    "args, _  = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if args.cuda and torch.cuda.is_available():    \n",
    "    device = \"cuda\"\n",
    "\n",
    "model = None\n",
    "if args.model == \"convNN2\":\n",
    "    model = convNN2().to(device)\n",
    "elif args.model == \"resnet18\":\n",
    "    model = resnet18().to(device)\n",
    "elif args.model == \"resnet34\":\n",
    "    model = resnet34().to(device)\n",
    "elif args.model == \"resnet50\":\n",
    "    model = resnet50().to(device)\n",
    "elif args.model == \"dense\":\n",
    "    model = denseNN(device).to(device)\n",
    "\n",
    "UTKFace = CustomImageDataset(args.train_file, 'UTKFace')\n",
    "train_dataloader = DataLoader(UTKFace, \n",
    "                                batch_size=args.batch, \n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training convNN2 from landmark_list.txt with batch_size=64\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28051/2676081613.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28051/2672724026.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, lr, device, valid_set, epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Validation is performed every 20 iterations due to its high overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28051/4191991630.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, valid_set_path, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# organise into (x, y) pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mland_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# The labels we are training for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28051/1664524918.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training {args.model} from {args.train_file} with batch_size={args.batch}\\n\")\n",
    "\n",
    "# Train model\n",
    "model, info, plots = train(model, train_dataloader, args.learning_rate, device, args.validation_file, epochs=args.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we save the model weights and model performance over time for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and training/validation results\n",
    "# filename includes batchsize, epoch number, learning rate\n",
    "filename = f\"{args.model}_{args.train_file}_batch{args.batch}_ep{args.epochs}_lr{args.learning_rate}\"\n",
    "model_path = f\"./models/{filename}.pt\"\n",
    "scores_path = f\"./model_scores/{filename}.csv\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "np.savetxt(scores_path, plots, delimiter=\",\")\n",
    "\n",
    "info[\"epochs\"] = args.epochs\n",
    "info[\"batch\"] = args.batch\n",
    "\n",
    "with open(f\"./model_infos/{filename}.json\", \"w\") as outfile:\n",
    "    json.dump(info, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network\n",
    "\n",
    "We can commence testing now that all the networks have been successfully trained. The models and training data are in the folders: *models*, *model_scores*, and *model_infos*.\n",
    "Each model will be tested and compared on three different sets: a 50-50 split of male to female testing data, a solely male testing set, and a solely female testing set.\n",
    "\n",
    "First we will load all the models into memory. This is simply done by insert them all into a list, initialising a model, and then loading each file into that subsequent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/oscarfzs/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/oscarfzs/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /home/oscarfzs/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for convNN2:\n\tUnexpected key(s) in state_dict: \"conv4.weight\", \"conv4.bias\", \"fc2.weight\", \"fc2.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([6, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28051/3313432009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/cudnn/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1668\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for convNN2:\n\tUnexpected key(s) in state_dict: \"conv4.weight\", \"conv4.bias\", \"fc2.weight\", \"fc2.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([6, 128]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([6])."
     ]
    }
   ],
   "source": [
    "# Adding in model names\n",
    "model_names = [\n",
    "        \"convNN2_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \n",
    "        \"convNN2_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\", \n",
    "        \"convNN2_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\", \n",
    "        \"dense_ll_training_50-50_split.txt_batch32_ep50_lr0.0001.pt\", \n",
    "        \"dense_ll_training_25-75_split.txt_batch32_ep50_lr0.0001.pt\",\n",
    "        \"dense_ll_training_75-25_split.txt_batch32_ep50_lr0.0001.pt\", \n",
    "        \"resnet34_ll_training_50-50_split.txt_batch64_ep50_lr0.0001.pt\", \n",
    "        \"resnet34_ll_training_25-75_split.txt_batch64_ep50_lr0.0001.pt\",\n",
    "        \"resnet34_ll_training_75-25_split.txt_batch64_ep50_lr0.0001.pt\"]\n",
    "\n",
    "models = [convNN2(), convNN2(), convNN2(), denseNN(\"cpu\"), denseNN(\"cpu\"), denseNN(\"cpu\"), resnet34(), resnet34(), resnet34()]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    model.load_state_dict(torch.load(\"./models/\" + name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "Once we have done this we can now calculate the scores of each model for every test set. This should compute the mean error and standard deviation and return it as a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    print(\"50-50 test file \" + evaluate(model, \"./ll_test_50-50\", \"cpu\"))\n",
    "    print(\"male test file \" + evaluate(model, \"./ll_test_males\", \"cpu\"))\n",
    "    print(\"female test file \" + evaluate(model, \"./ll_test_females\", \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its simplicity, the error for the convNN2 is much higher than the the other two models. \n",
    "While we can see that ResNet34 outperformed DesneNet121 with a significant reduction in error between the two, \n",
    "both models had higher error in females for identifing landmarks for both genders, as expected at the start of this project.\n",
    "\n",
    "Both models performed best overall on balanced data sets, though this difference was very slight. It also shows that a higher proportion of females or males in the dataset does not necessarily result in a lower error for each respective gender. However, the change in error was quite minimal, meaning we can discard the bias in the dataset as a source of the bias between males and females.\n",
    "\n",
    "Now we can generate graphs for the change in mean error and standard deviation during training, and how it progressed over the iterations.\n",
    "When loading in the model scores file, we need to parse this data to make it useful to us. Namely, we split it into a list by using ',' as dividers.\n",
    "The model infos are already presented in json format, and can simply be read in directly. This gives us the iteration, mean, and standard deviation of the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model_scores/convNN2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28051/4155163751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mm_nopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model_scores/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm_nopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_scores/convNN2.csv'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model in models:\n",
    "    epoch = []\n",
    "    error = []\n",
    "    std = []\n",
    "    m_nopt = args.model.split(\".pt\")\n",
    "    with open(\"./model_scores/\" + m_nopt[0] + \".csv\") as file:\n",
    "        for line in file:\n",
    "            scores = line.split(\",\")   \n",
    "            epoch.append(float(scores[0]))\n",
    "            error.append(float(scores[1]))\n",
    "            std.append(float(scores[2]))\n",
    "    \n",
    "    file = open(\"./model_infos/\" + m_nopt[0] + \".json\")\n",
    "    model_info = json.load(file)\n",
    "\n",
    "    plt.plot(epoch, error)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Mean Error\")\n",
    "    plt.title('Error change during training')\n",
    "\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"mean\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch, std)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Standard Deviation\")\n",
    "    plt.title('STD change during training')\n",
    "    plt.scatter([model_info[\"iteration\"]], [model_info[\"std\"]], color = 'red')\n",
    "    plt.gca().legend(('Validation Error','Best performing iteration'))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs we can see that the mean error and standard deviation rapidly decreases over time within the first few iterations. Initially, with the weights initialized to zero, the predicted landmarks all fall very far from the correct points.\n",
    "\n",
    "Both the ConvNN2 and DenseNet seem to oscillate between certain points and do not converge to a single point like the Resnet34 does. This seems to indicate that they are not necessarily the most optimal model.\n",
    "\n",
    "We can also see the loss during training with the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph for loss during training\n",
    "for model in models:\n",
    "    plt.plot(list(range(len(model_info[\"loss_list\"]))), model_info[\"loss_list\"])\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title('Loss during training')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually check how accurate the predicted landmarks are on an image by comparing them against the input landmarks used by the model for training. For each model an image will be generated that plots 2 lines, with 5 images per line for easy comparison between different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(train_dataloader, axs_flat):\n",
    "    with torch.no_grad():\n",
    "            for i, (image, _, _, _, labels) in enumerate(train_dataloader):\n",
    "                output = model(image)\n",
    "                output = output.reshape(3,2)\n",
    "                image = image.squeeze()\n",
    "                image = image.permute(1, 2, 0)    #Default was 3,200,200\n",
    "                im = axs_flat[i].imshow(image)\n",
    "                x = np.array(range(200))\n",
    "\n",
    "                # Finding landmarks for chin, nose and eye this txt file for each image\n",
    "                land_idx = [8, 30, 39]\n",
    "                labels = labels.squeeze()\n",
    "                labels = labels[land_idx, :]\n",
    "                #ax.scatter(output[:,0], output[:,1], linewidth=2, color='red')\n",
    "                axs_flat[i].scatter(output[:,0], output[:,1], linewidth=2, color='c', s = 5)\n",
    "                axs_flat[i].scatter(labels[:,0], labels[:,1], linewidth=2, color='m', s = 5)\n",
    "\n",
    "UTKFace = CustomImageDataset(\"./test_output_images\", 'UTKFace')\n",
    "train_dataloader = DataLoader(UTKFace, \n",
    "                                    batch_size=1, \n",
    "                                    shuffle=False)\n",
    "\n",
    "for model in models:\n",
    "    print(model.__class__.__name__)\n",
    "    fig, axs = plt.subplots(2,5, figsize=(20,10))\n",
    "    axs_flat = axs.flatten()\n",
    "\n",
    "    generate_images(train_dataloader, axs_flat)\n",
    "    #plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    fig.legend(('Predicted output','Expected output'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "As we can see the basic convolutional neural network essentially predicts the same three points for every single image. While that does produce the best possible mean error for this specific model, the results do show that the model is severely underfitting. It does not have enough layers, depth, or complexity to recognize the different landmark features on each face.\n",
    "\n",
    "Currently, the model produces a very high mean error (54.377 and 54.895 pixels for females and males, respectively). On a 200x200 pixel image this is very far off. Thus, convNN2 will not be considered when evaluating our hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ResNet34 and DenseNet121 vastly outperformed convNN2 and get significantly lower average mean error and standard deviation.\n",
    "Their errors on different datasets can be seen below\n",
    "##### ResNet34 Test Data\n",
    "![ResNet34 Test Data](jupyter_images/resnet34table.jpg) \n",
    "##### DenseNet121 Test Data\n",
    "![DenseNet121 Test Data](jupyter_images/densenet121table.jpg)\n",
    "\n",
    "From the tables we can see that ResNet had a mean error about 45% lower than DenseNet, which was a lot more sensitive to data bias from the data sets.\n",
    "The DenseNet had a higher error for women regardless of dataset bias. The error on the female test set was also very insensitive to dataset bias, indicating that the model is not properly learning how to predict the correct landmarks for women.\n",
    "\n",
    "We can look at a graph for all three models to get a better understand of what is happening during training.\n",
    "\n",
    "##### convNN2 Loss\n",
    "![convNN2 loss](jupyter_images/lossconnv2.jpg)\n",
    "##### DenseNet121 Loss\n",
    "![densenet121 loss](jupyter_images/lossdense.jpg)\n",
    "##### ResNet34 Loss\n",
    "![resnet34 loss](jupyter_images/lossresnet.jpg)\n",
    "\n",
    "\n",
    "Another issue may be due to underfitting of the model, which may be experimented with by increasing the amount of layers in the network in the future. If there is little to no increase in mean error, it would highlight that vanishing gradients are at play and that the number of layers must be reduced instead.\n",
    "\n",
    "![Both Models Mean Error Change](jupyter_images/modelgraphs.jpg)\n",
    "\n",
    "ResNet mean error converges asymptotically, while DenseNet continues oscillating. This indicates that the DenseNet model had issues during training and did not ouput a very optimal model. High oscillation tends to indicate that the learning rate is too high, or that the training might benefit from using an optimiser with momentum. DenseNet is ostensibly a more sophisticated model as it has both more layers and parameters, but this did not appear to hold in practice. It appeared the model was unable to properly fit to the female portion of the dataset, while ResNet despite being comparatively simpler, was able to. We theorise this is due to the DenseNet having vanishing gradient issues due to its high layer count. While DenseNet is intended to prevent vanishing gradients, the pytorch implementation only uses dense blocks. The layers inside these blocks possess dense connections, but subsequent layers only receive inputs from the previous dense block's output layer. This saves on computational complexity by bringing down the number of weights. However, this could allow vanishing gradients to exist. The ResNet model however used fewer layers and took advantage of skip connections. Since it uses summation as opposed to concatenation to bypass layers, it is more computationally efficient. Due to this, the model does not need to be split into blocks and the outputs from earlier layers can reach all later layers. This theory would explain our concerns about vanishing gradients. Additionally, when we examined the model weights for our trained DenseNets, we found that many of the earlier weight parameters had changed by no more that 1e-4, further supporting the vanishing gradient issues.\n",
    "\n",
    "As discussed, research suggests that discrepencies between men and women in facial recognition are not due to dataset bias but rather differences in facial structure. Another difference between ResNet and denseNet is their choice of pooling algorithms. Max pooling is likely better at learning to find edges and contours, while average pooling has a propensity to smooth values. If we assume that women tend to have softer features and therefore less defined contours on their face, it may be harder for a model using average pooling to identify these contours. Since men tend to have more defined features with more contrast, the model may cope better. This is reflected in our models as DenseNet performed worse than ResNet and Resnet features many more max pooling layers while densenet contains many average pooling layers.\n",
    "\n",
    "While underfitting can also be a symptom of poor hyperparameter selection or insufficient data, we do not believe this to be the case. The models were both trained on large datasets and we had spent some time testing  hyperparameters that would produce satisfactory results for all models.\n",
    "\n",
    "#### Further Discussion\n",
    "\n",
    "Clearly, the inherent differences between male and female facial structure can pose problems when developing facial recognition models. We have shown this is largely independent of any dataset bias. While we have identified the cause of these issues, further work would be required to determine the extent of this bias. Since we have only tested on three landmarks, we are unable to determine if these issues affect all landmarks or if there is a small subset that are responsible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "\n",
    "* Investigating which landmarks most heavily influence gender bias in model predictions may be a good next step. It would help affirm our conclusions if landmarks with less contrast were more biased, as it would support our theory that it is the difference in facial definition causing the bias.\n",
    "\n",
    "* Examining different pooling algorithms would also help support our results. This could be done by training a version of denseNet with max pooling instead of average pooling between each dense block, and a version of Resnet with without max pooling. If the denseNet performance improves while the resnet decreases, this would support the literature.\n",
    "\n",
    "* Testing different proportions of men and women in datasets would allow us to better understand how influenced the models are by bias and experiment with strategies and guidelines to make the models more resiliant to dataset bias. Due to time constraints this team only experimented with three different proportions, but more datapoints would provide considerably more information.\n",
    "\n",
    "* Investigate the minimal number of facial features necessary to perform face recognition fairly.​\n",
    "\n",
    "* This team would also like to suggest identifying how large of an impact vanishing gradients played on these results by performing the same tests with a smaller version of denseNet. If the scores increase or remain consistent that would point heavily to vanishing gradients as it indicates the model is overly complex.\n",
    "\n",
    "* This team is also interested on performing a similar experiment with different racial groups. Here we believe dataset bias plays a bigger role as datasets created in certain countries are likely to have the same distributions of ethnicities as their local populace. We also expect differences in skin tone and again, facial structure would also contribute. This experiment was cut for time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Our experiments thus far have provided promising insights into why facial recognition performs worse for certain groups, and more specifically genders. We were able to pinpoint differences in facial structure as a major contributing factor and some model design choices such as pooling algorithms that may exacerbate these issues. Additionally, we were able to find a model that was relatively unbiased while still producing low errors. These findings have helped verify existing research and can help influence future facial recognition development to be fairer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
